{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1e56fa-162b-490c-9379-5f402b7a43ad",
   "metadata": {},
   "source": [
    "# Building Agentic RAG with Llamaindex\n",
    "\n",
    "[deeplearning.ai course link here](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/1/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b6d64-3b0c-4b48-9090-57707a7d5751",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***\n",
    "\n",
    "* Jerry Liu: co-founder and CEO of LlamaIndex and instructor of course.\n",
    "\n",
    "* Will learn about agentic RAGs, a framework to help you build research agents, capable of doing reasoning, decision making over your data.\n",
    "\n",
    "* Standard RAG pipeline is mostly good for simpler questions over a small set of documents, works by retrieving some context and sticking to an LLM prompt and calling it a single time to get a response.\n",
    "\n",
    "* We will build an autonomous research agent. Will learn **a progression of reasoning ingredients** to build a full agent:\n",
    "\n",
    "    * **Routing**: We add decision making to route requests to multiple tools\n",
    "    * **Tool-use**: Create interface for agents to select a tool and generate the right arguments for that tool.\n",
    "    * **Multi-step reasoning**: Use an LLM to perform reasoning with a range of tools for retaining memory throughout that process.\n",
    "\n",
    "\n",
    "* Let user optionally inject guidance at intermediate steps (e.g. guiding when searching a document). Just like an experienced manager giving a junior employee a nudge to consider a new piece of info to achieve much better performance.\n",
    "\n",
    "* **First lesson**: Build a router over a single document that can handle question-answering and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48a148-9de9-47d0-b195-c203bf470fd6",
   "metadata": {},
   "source": [
    "## Router Query Engine\n",
    "***\n",
    "\n",
    "Simplest form of agentic RAG. Given a query, router picks one of several query entrants (ways) to execute a query. Will build a single router for a single document to do question-answering and summarization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e476d03-4b96-4c30-9d7d-5010cea1803c",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/router_engine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dc8ce-8770-45f4-b27c-ab88ff4725ae",
   "metadata": {},
   "source": [
    "Load OpenAI API Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "069da77b-46b8-4750-bb6d-983e089e202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Accessing the environment variable\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded properly\n",
    "if openai_api_key is not None:\n",
    "    print(\"OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load OPENAI_API_KEY. Please check if it is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32535534-bf44-438d-b768-165db434b043",
   "metadata": {},
   "source": [
    "Jupyter runs an event loop behind the scenes, and a lot of our modules use async; to make async play nicely with Jupyter Notebooks we need this `nest_asyncio` package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b478cc5-fd0d-4b11-8cea-5e4e62835f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446489c-7f2a-41c7-a840-eccb64c8e18e",
   "metadata": {},
   "source": [
    "Next is to load in a sample PDF document. Read PDF into a **parsed document representation**.\n",
    "\n",
    "We will use the `SimpleDirectoryReader` module and *LlamaIndex* to read in this PDF into a parsed document representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e6e96cb-e460-4e49-adcb-4716717d7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full path to the file is: data/pdfs/P1-mher-arabian.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Define your base directory\n",
    "base_dir = 'data/pdfs'\n",
    "\n",
    "# Define your file name\n",
    "file_name = 'P1-mher-arabian.pdf'\n",
    "\n",
    "file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "print(\"The full path to the file is:\", file_path)\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[file_path]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb666816-f131-4dd2-8db6-02e6189cfc04",
   "metadata": {},
   "source": [
    "Next we'll split documents into even size chunks using the `SentenceSplitter`, we'll split on the order of sentences. We set the chunk size to 1024. \n",
    "\n",
    "We call `splitter.get_nodes_from_documents()` to split these documents into **nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1bc0e64-6325-4c56-8857-c7a5ebdefe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3813e39f-2d18-4875-a5c7-834dba754725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of nodes object is: <class 'llama_index.core.schema.TextNode'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of nodes object is: {type(nodes[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8293f63-5300-4507-aea5-8efccef5ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the nodes list is: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of the nodes list is: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af2f28-8012-4662-b552-142c7edef8ce",
   "metadata": {},
   "source": [
    "This next step allows us to define an LLM and embedding model. Can do this by specifying a global config setting where you specify the LLM and embedding model that you want to inject as part of the global config.\n",
    "\n",
    "By default we use 3.5 turbo and text embedding ada-002 in this course. This allows you to have the groundwork to inject your own LLMs and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "556f2a82-0218-4ca1-bf13-f57fddd3b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de946115-8407-465f-b3b8-cc6f764c7ef8",
   "metadata": {},
   "source": [
    "Now we are ready to start building some indexes. Here we define two indexes, over these nodes:\n",
    "\n",
    "* Summary Index\n",
    "* Vector Index\n",
    "\n",
    "Can think of an index as a set of of metadata over our data.  You can query an index, and different indexes will have different retrieval behaviors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90e416-c6b9-47b9-a51a-03397897e38e",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/vector_index_vs_summary_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745db65b-e559-44bb-b8e1-c931be4ad909",
   "metadata": {},
   "source": [
    "### Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52baf2-0f49-4764-a6b0-fb43535c13c7",
   "metadata": {},
   "source": [
    "A **vector index** indexes nodes via text embedings and is a core abstraction in LlamaIndex and in any sort of RAG system. \n",
    "\n",
    "Querying a vector index will return the most similar nodes by embedding similarity.\n",
    "\n",
    "\n",
    "![Router Engine](imgs/vector_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586763b2-8c6c-4572-8042-31e3aa175384",
   "metadata": {},
   "source": [
    "### Summary Index\n",
    "\n",
    "* Very simple index -  querying it will return all the nodes currently in the index. So it doesn't depend on the user query, but will return all the nodes currently in the index.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57896d05-1983-423f-a433-576a1e255c87",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/summary_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081185d0-3c90-4034-8e2b-1c94d9574b5b",
   "metadata": {},
   "source": [
    "### Setting Them Both Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a69f3f3-1f62-441a-98a2-fcc96ab53cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eab598-d3d0-4e04-80b9-0ecaa31d1f15",
   "metadata": {},
   "source": [
    "Now let's turn these indexes into **query engines** and then **query tools**:\n",
    "\n",
    "* Each query engine represents an overall query interface over the data that's stored in that index. It **combines retrieval with LLM synthesis**.\n",
    "\n",
    "* Each query engine is good for certain type of questions - great use case for a **router** which can route dynamically between these different query engines.\n",
    "\n",
    "* A **query tool** is now just a query engine with metadata, specifically a description of what types of questions the tool can answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd4b68ea-3b14-46aa-9a9f-7f51f7316ad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary_query_engine \u001b[38;5;241m=\u001b[39m \u001b[43msummary_index\u001b[49m\u001b[38;5;241m.\u001b[39mas_query_engine(\n\u001b[1;32m      2\u001b[0m     response_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_summarize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m vector_query_engine \u001b[38;5;241m=\u001b[39m vector_index\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_index' is not defined"
     ]
    }
   ],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbbac9-d905-4ad7-bea6-d568945b1b86",
   "metadata": {},
   "source": [
    "* Can see that the query engines are derived from each of the indexes.\n",
    "\n",
    "* Can see we use `use_async=True` for the summary query engine to basically enforce faster query generation by allowing async capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409d200-14d3-4ff8-83bd-123066971448",
   "metadata": {},
   "source": [
    "Next a **query tool** is just a query engine with some metadata -specifically a description of what type of question(s) that tool can answer.\n",
    "\n",
    "We'll define a **query tool** for both the *summary* and *vector* query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4122022a-ba92-4ee6-9564-774fe6976a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to `Assignment P1 CS 6675: Advanced Internet Systems and Applications report.`\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from `Assignment P1 CS 6675: Advanced Internet Systems and Applications report.`\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87127dc-ad18-46a7-b884-c306df67a069",
   "metadata": {},
   "source": [
    "### Selectors\n",
    "\n",
    "Now ready to define our **Router**. LlamaIndex provides several different types **Selectors** to enable you to build a router, each of these selectors have distinct attributes.\n",
    "\n",
    "There are several selectors available:\n",
    "\n",
    "* The **LLM selectors** use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.\n",
    "\n",
    "* The **Pydantic selectors** use the OpenAl Function\n",
    "Calling API to produce pydantic selection objects, rather than parsing raw JSON.\n",
    "\n",
    "\n",
    "\n",
    "For each of these kinds of selectors, also have dynamic capabilities to select one index to route to, or actually multiple.\n",
    "\n",
    "Let's try an LLM powered single selector - called `LLMSingleSelector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb41028-989f-4cba-804e-aa6cefa1006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07850b46-ed0b-4991-a643-795366864ac6",
   "metadata": {},
   "source": [
    "We import two modules:\n",
    "* **RouterQueryEngine**\n",
    "* **LLMSingleSelector**\n",
    "\n",
    "`RouterQueryEngine` takes in **selector type** as well as a set of **query engine tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f4afd-4af9-4cc9-a719-8ec64aa35c72",
   "metadata": {},
   "source": [
    "Let's test out some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67644323-cc46-40b5-b9e6-2327ad051c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The summary of the document is related to summarization questions, making choice 1 the most relevant..\n",
      "\u001b[0mThe document discusses various Internet-fueled innovations such as cloud computing, smart-watches, and community-based chat platforms. It highlights how these innovations automate tasks for users and involve cloud computing in some way. Additionally, it explains the differences between smart-watches and other innovations, emphasizing the unique hardware aspect of smart-watches. The document also delves into the concepts of surface web and deep web activities, explaining the challenges of crawling dynamic URLs in the deep web. Lastly, it touches upon limitations of a specific crawler project and proposes strategies for designing a crawler for a university website.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa163fb-480f-4969-b269-f47a926dc665",
   "metadata": {},
   "source": [
    "Verbose output allows us to view the intermediate steps that have been taken. We see that the output includes:\n",
    "\n",
    "* Selecting query engine 0: Useful for summarization questions. This means the first option was picked to help answer this question. As a result - you are able to get back a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ae80-abb3-4325-83b4-648c2c71af23",
   "metadata": {},
   "source": [
    "The response comes with **sources**. Let's take a look using `response.sources_nodes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a079b9-9253-41b1-a8ba-2e2e3000b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc8cf6-9424-4d64-b93d-db31e88f86c8",
   "metadata": {},
   "source": [
    "Exactly equal to the number of chunks of the entire document. We see the summary query engine must have been getting call - because the **summary query engine returns all the chunks corresponding to the items within its index**.\n",
    "\n",
    "Let's take a look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8709fb2d-ee50-4c6a-a8d6-62fa8ef6db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the report, which includes information about the author..\n",
      "\u001b[0mMher Arabian.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who write this paper?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317d4c2-703d-497c-aa67-67751c2a8d05",
   "metadata": {},
   "source": [
    "* Used vector search tool, as opposed to the summary tool.\n",
    "\n",
    "* Here focus is on retrieving specific context from the PDF - especially when information is located within a paragraph of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4345f-c4c5-41ac-8111-80601834cce4",
   "metadata": {},
   "source": [
    "**Puting everything together**: \n",
    "* Single helper function that takes in a file path and builds a router query engine with both vector search and summarization over it.\n",
    "* See `get_router_query_engine` in *utils.py* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4700781-82ee-46fd-acc1-9f04881f6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "# Load paper \"METAGPT: META PROGRAMMING FOR A MULTI-AGENT COLLABORATIVE FRAMEWORK\"\n",
    "file_path = os.path.join('data/pdfs', 'metagpt.pdf')\n",
    "query_engine = get_router_query_engine(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ca2d7a-fdc4-4299-ae17-f933dbf49834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results demonstrate the effectiveness of MetaGPT in addressing challenges related to context utilization, reducing hallucinations in software generation, and managing information overload. The study highlights how MetaGPT's unique designs successfully unfold natural language descriptions accurately, maintain information validity in lengthy contexts, and reduce code hallucination problems. Additionally, the study showcases the use of a global message pool and subscription mechanism to streamline communication and filter out irrelevant information, ultimately enhancing the relevance and utility of the generated information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10ca144-8e5e-49d5-b7a3-dc3aa86f0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
      "\u001b[0mThe paper introduces MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). MetaGPT focuses on improving code generation quality through role specialization, workflow management, and efficient communication mechanisms. It outperforms existing approaches in various benchmarks, demonstrating its effectiveness in software development tasks. The framework uses natural language programming to generate software code by involving different agents like the Architect, Engineer, and QA Engineer. MetaGPT aims to simplify the process of transforming abstract requirements into detailed designs by efficiently dividing tasks. The paper also addresses challenges such as reducing code hallucinations and handling information overload, while discussing ethical concerns like skill obsolescence, transparency, and privacy in the context of natural language programming's potential to enhance software development processes.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Summarize the paper for me.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64ef51-56f4-49df-994b-61c1ea6443dc",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df700da-72bc-4e10-8953-493d865ffad5",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "* In a basic RAG pipeline, LLMs are only use for synthesis. The previous lesson showed you how to use LLMs to make a decision by picking a choice of different pipelines.\n",
    "\n",
    "* This is a simplified form of tool calling. In this lesson we will learn how to use an LLM to not only pick a function to execute, but also infer an argument to pass to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191be46f-1dc6-47bf-be01-03d920ff0829",
   "metadata": {},
   "source": [
    "One of the Promises of LLMs:\n",
    "\n",
    "* Ability to take actions/interact with external environment. For this, we need a good interface for the LLMs to use.\n",
    "\n",
    "* We call this **Tool Calling**.\n",
    "\n",
    "\n",
    "* In previous lesson we saw how to use LLMs in a slightly more sophisticated matter than just synthesis. By using it to pick the best query pipeline to answer the user query. In this lesson we will learn how to use LLMs to not only pick a function to execute - but also infer an argument to pass to that function.\n",
    "\n",
    "* Allows LLM to figure out how to use a vector DB, instead of just consuming its outputs.\n",
    "\n",
    "* Tool calling enables LLMs to interact with external environments through a dynamic interface where tool calling not only helps choosing the appropriate tool but also infer necessary arguments for execution.\n",
    "\n",
    "\n",
    "* Tool calling adds a layer of query understanding on top of a RAG pipeline, enable users to ask complex queries and get\n",
    "back more precise results.\n",
    "\n",
    "\n",
    "* Final result: Users are able to ask more questions, and get back more precise results than standard RAG techniques thru tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb728d-84b5-4d0c-8c03-34099933e892",
   "metadata": {},
   "source": [
    "Let's see how to define a tool interface from a python function. LLM will infer the parameters from the signature of the python function using LlamaIndex abstractions.\n",
    "\n",
    "Let's see how Tool Calling works using two toy calculator functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca65a631-171c-4f76-bc85-7813a96aea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a378298-a41c-475b-8084-613584ae7f39",
   "metadata": {},
   "source": [
    "Core abstraction is **FunctionTool**, wraps any given python function you feed it, here it takes in both the add function as well as the mystery function.\n",
    "\n",
    "You see both have type annotations for both x,y variables and docstrings. These things are not just for stylistic purposes, they will actually be used as a prompt for the LLM.\n",
    "\n",
    "LlamaIndex FunctionTools integrate natively with the function calling capabilities of many different LLMs.\n",
    "\n",
    "To pass the tools to an LLM, have to import the LLM module and call `predict_and_call`.\n",
    "\n",
    "\n",
    "Here we import the OpenAI module explicitly, and we see the model is 3.5 turbo - we call the `predict_and_call` function on top of the LLM.\n",
    "\n",
    "What predict and call does:\n",
    "* Takes in a set of tools, as well as an input prompt string, or a series of chat messages, and then its able to both make a decision of the tool to call, as well as call the tool itself and get back the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f769d1-e603-4267-aa29-f6df1c701113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4743212a-481d-4df0-a92e-f1f21db39330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"x\": 10, \"y\": 1}\n",
      "=== Function Output ===\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of add function with 10 and 1\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24752cc-23ac-4237-b0b5-cb4c12742e6b",
   "metadata": {},
   "source": [
    "We see intermediate steps here - calling function mystery with the arguments x=2 and y=9. We see called the right tools, and inferred the right parameters. The output is 121.\n",
    "\n",
    "**Note**: This simple example is effectively an expanded version of the router. Not only does the LLM pick the tool but also decides what params to give it.\n",
    "\n",
    "Let's define a slightly more sophisticated agentic layer on top of vector search. Not only can the LLM choose vector search, but **we can also get it to infer metadata filters** - which is a structured list of tags that helps return a more precised set of search results.\n",
    "\n",
    "Let's pay attention to the actual nodes themselves - the chunks. Because we'll take a look at the actual metadata attached to these chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719387a1-2a02-4a3f-9fc5-f421565d1368",
   "metadata": {},
   "source": [
    "Again we will use the `SimpleDirectoryReader` module to load in the parsed reprentation of this PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3216e10d-fe20-40be-9b8c-795925c6698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "file_path = os.path.join('data/pdfs', \"metagpt.pdf\")\n",
    "documents = SimpleDirectoryReader(input_files=[file_path]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab189c-3a0f-48ec-85f9-5c44928e2370",
   "metadata": {},
   "source": [
    "Next will split these documents into a set of even chunks, with a chunk size of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c0b2d8-a927-4752-9434-67567df10b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba2b3d68-9dbb-43e3-8ba4-b46d2f8d1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f667870-33b1-475b-9c55-483f0ddd4e7a",
   "metadata": {},
   "source": [
    "Each node represents a chunk, let's look at content of an example chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f598abc-f2b4-4ebf-9752-daab8f6ac6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: data/pdfs/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-06-07\n",
      "last_modified_date: 2024-06-07\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# \"all\" is a special setting - not just enable to print content of node\n",
    "# but also the metadata attached to the doc, which is propagated to every node\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9da219cc-9423-42f1-8936-3f6e0413a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint\n",
      "Figure 2: An example of the communication protocol (left) and iterative programming with exe-\n",
      "cutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\n",
      "They can also subscribe to relevant messages based on their profiles. Right : After generating the\n",
      "initial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\n",
      "messages stored in memory and compares them with the PRD, system design, and code files.\n",
      "3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\n",
      "MetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\n",
      "vides an explanation of role specialization, workflow and structured communication in this frame-\n",
      "work, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\n",
      "presents a communication protocol that enhances role communication efficiency. We also imple-\n",
      "ment structured communication interfaces and an effective publish-subscribe mechanism. These\n",
      "methods enable agents to obtain directional information from other roles and public information\n",
      "from the environment. Finally, we introduce executable feedback—a self-correction mechanism for\n",
      "further enhancing code generation quality during run-time in Sec. 3.3.\n",
      "3.1 A GENTS IN STANDARD OPERATING PROCEDURES\n",
      "Specialization of Roles Unambiguous role specialization enables the breakdown of complex work\n",
      "into smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\n",
      "oration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\n",
      "specific issues.\n",
      "In a software company, a Product Manager typically conducts business-oriented analysis and derives\n",
      "insights, while a software engineer is responsible for programming. We define five roles in our\n",
      "software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\n",
      "shown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\n",
      "goal, and constraints for each role. We also initialize the specific context and skills for each role.\n",
      "For instance, a Product Manager can use web search tools, while an Engineer can execute code, as\n",
      "shown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\n",
      "Every agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\n",
      "servations ( e.g.,, messages from other agents). These messages can either directly trigger actions or\n",
      "assist in finishing the job.\n",
      "Workflow across Agents By defining the agents’ roles and operational skills, we can establish\n",
      "basic workflows. In our work, we follow SOP in software development, which enables all agents to\n",
      "work in a sequential manner.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# all is a special setting - not just enable to print content of node\n",
    "# but also the metadata attached to the doc, which is propagated to every node\n",
    "print(nodes[4].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06500d93-dbe6-4ad0-9a15-dcb43586a066",
   "metadata": {},
   "source": [
    "Notice how it added a `page_label` annotation to each chunk.\n",
    "\n",
    "Next let's define a **vector store** index over these nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92d90a14-a3b6-4dc3-93e7-29f7705b0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf120c7-2fd9-479d-88d5-c3cacc859a86",
   "metadata": {},
   "source": [
    "### Querying RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5684c-7e16-4d02-9eb7-a0edf9334686",
   "metadata": {},
   "source": [
    "This will build a RAG indexing pipeline over these nodes. Will add an embedding for each node, and it will get back a query engine.\n",
    "\n",
    "Differently from last time, we can try querying this RAG pipeline via metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d6e263f-21a1-48d5-b7e1-74cf7d7c2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some of the high-level results of MetaGPT?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "346f8a90-7311-4efb-957b-4b36ea15e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality. Additionally, MetaGPT demonstrates a 100% task completion rate in experimental evaluations, showcasing its robustness and efficiency in terms of time and token costs.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfdd25e0-5b77-4a37-bbbb-57018a4eaea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069ae2c-d382-4686-b1ef-960e90e8f5d4",
   "metadata": {},
   "source": [
    "### Enhanced Data retrieval\n",
    "\n",
    "* Integrating Metadata Filters into a retrieval tool function.\n",
    "\n",
    "* This function enables more precise retrieval retrieval by accepting a query string and optional metadata filters, such as page numbers.\n",
    "\n",
    "* The LLM can intelligently infer relevant metadata filters (e.g., page numbers) based on the user's query.\n",
    "\n",
    "* You can define different type of metadata filters like section IDs, headers, or footers,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1436c6-6a82-496d-9f8a-0465cfe7b76e",
   "metadata": {},
   "source": [
    "Function below takes in a query as well as page numbers. This allows you to perform a vector search over an index, along with specifying page numbers as a metadata filter.\n",
    "\n",
    "At the very end we define a vector query tool. We pass in the vector query function into the vector query tool - which allows us to then use it with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462c525b-4e3c-4ca7-a89d-cb10cfc44524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dc744-c0b4-41e7-8187-bb8219015f56",
   "metadata": {},
   "source": [
    "Let's call this tool with an LLM. It should be able to infer both the string as well as the metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20e73901-f45c-49c3-aa07-7b45693c06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating a 100% task completion rate in experimental evaluations.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"What are the high-level results of MetaGPT as described in page 2\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256ac0d-ef2d-40d9-8447-97b767efdc90",
   "metadata": {},
   "source": [
    "LLM formulates the right query, as well as specify the page numbers. We get back the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4e6ff-71d4-49c2-a6c3-b61c0e0f78be",
   "metadata": {},
   "source": [
    "Let's verify the source nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56ee1014-4b66-4cd0-84da-83547f275523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38b03d-bc87-4197-818e-4f43d4c5b44d",
   "metadata": {},
   "source": [
    "Finally, we can bring in the summary tool from the router example of the first lesson and we can combine that with the vector tool, to create this **overall tool picking system**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3330456-53be-4989-bdbe-415127c3dc0f",
   "metadata": {},
   "source": [
    "This code sets up a summary index over the same set of nodes and wraps it in a summary tool similar to lesson 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab41309-4cb9-4665-9e0e-6762fcc4eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4445804-47ca-45b4-a9e0-dca914c5c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev on the challenging SoftwareDev dataset in nearly all metrics. For example, MetaGPT achieves a higher score in executability, takes less time for execution, requires more tokens for code generation but needs fewer tokens to generate one line of code compared to ChatDev. Additionally, MetaGPT demonstrates superior performance in code statistics and the cost of human revision when compared to ChatDev.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c405de-0568-4c29-b773-793c095b0d88",
   "metadata": {},
   "source": [
    "Now LLM has slightly harder task - must pick the right tool in addition to inferring the function parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe34668-e1ed-47f4-bac6-e0978cf9eb3c",
   "metadata": {},
   "source": [
    "Let's verify this by printing out the sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74953632-927b-4ea4-9a4e-94ac3273dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce328ffe-5bb7-43a2-ab8e-07a790696eb7",
   "metadata": {},
   "source": [
    "Let's now ask a question, to show that the LLM can still pick the summary tool when necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc95e0e1-2559-4ed3-b959-5b3eee853bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It introduces role specialization, structured communication interfaces, and executable feedback mechanisms to improve code generation quality. In experiments, MetaGPT surpassed previous approaches in various benchmarks, showcasing its effectiveness in software development tasks. The framework also introduces innovative concepts like self-improvement mechanisms and multi-agent economies for future research and development. The Architect agent devises technical specifications based on the Product Requirement Document (PRD), while the Project Manager breaks down tasks and assigns them to Engineers. The Engineer agent requires fundamental development skills, and the QA Engineer generates unit test code to ensure high-quality software. Ultimately, MetaGPT generates a functional application called \"Drawing App\" using Python's Tkinter and PIL (Pillow) libraries for GUI and color selection functionality, respectively.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is a summary of the paper?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a1f7f-597c-4d73-ab7d-b25f487d1b33",
   "metadata": {},
   "source": [
    "Next lesson: How to build a full agent over a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e8f53-a425-4822-9b71-64300c9be59d",
   "metadata": {},
   "source": [
    "## Building an Agent Reasoning Loop\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6967e37-69f2-47ed-b5e5-a87c2bc51494",
   "metadata": {},
   "source": [
    "In this lesson, will learn how to define a complete agent reasoning loop. Instead of tool calling in a single-shot setting, an agent is able to reason over tools in multiple steps.\n",
    "\n",
    "Will use the function calling agent implementation which is an agent that natively integrates with the function calling capabilities of LLMs.\n",
    "\n",
    "Let's have some fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1aa2e7b-8a38-4edd-a2c6-a9f8ab22719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Accessing the environment variable\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded properly\n",
    "if openai_api_key is not None:\n",
    "    print(\"OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load OPENAI_API_KEY. Please check if it is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4031e30-f418-4467-957b-a9da932de71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary setup to run in notebook environment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c3a1d-6251-4bf0-90ad-9c26a2088959",
   "metadata": {},
   "source": [
    "Let's also setup the autoretrieval vector seach tool and summarization tool from the last lesson.\n",
    "\n",
    "Made it easy with a function in the utils file. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ba7052f-1143-4bc5-bbe2-58a068bb2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_doc_tools\n",
    "\n",
    "# make sure you have a valid pdf file in this directory\n",
    "file_path = os.path.join('data/pdfs', 'metagpt.pdf')\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(file_path, \"metagpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f690b31-d978-4d33-9a4a-1782bfe74802",
   "metadata": {},
   "source": [
    "### High-level Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46df6c-cc71-461f-a858-e93e1cffda5d",
   "metadata": {},
   "source": [
    "Let's setup our function calling agent.\n",
    "\n",
    "\n",
    "In LlamaIndex, an agent consists of two main components:\n",
    "1. **Agent Worker**: Responsible for executing the next step of a given agent.\n",
    "2. **Agent Runner**: Overall task dispatcher, responsible for creating a task, orchestrating runs of agent workers on top of a given task, and being able to return back the final response to the user.\n",
    "\n",
    "\n",
    "![Agent Intro](imgs/agent_intro.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "606c0d2c-e671-444e-86aa-ccf40628964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55accd93-5dea-4f77-8cf7-4e5207742079",
   "metadata": {},
   "source": [
    "To the `FunctionCallingAgentWorker`, we pass in two set of tools:\n",
    "\n",
    "* Vector tool\n",
    "* Summary tool\n",
    "\n",
    "We also pass in the LLM and set verbose equal to true to look at the intermediate outputs.\n",
    "\n",
    "Think about the `FunctionCallingAgentWorker`'s primary responsibility as given the existing converstion history, memory, and any past state, along with the current user input: \n",
    "1. **Use function calling to decide the next tool to call**\n",
    "2. **Call that tool**\n",
    "3. **Decide whether or not to return a final response**\n",
    "\n",
    "The overall agent interface is behind the **agent runner**, and **that's what we're gonna use to query the agent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34a4e77b-6746-4390-9e6c-d96a1ea2cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain the book Human Physiology by Fox\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"Explain the book Human Physiology by Fox\"}\n",
      "=== Function Output ===\n",
      "The book \"Human Physiology\" by Fox is currently in its sixteenth edition and is published by McGraw Hill LLC. It aims to help students understand the concepts of human physiology in a readable, current, and student-oriented manner. The sixteenth edition has been updated with contributions from Krista Rompolski, Ph.D., from Moravian College, who significantly revised chapters 8 and 18. The book maintains its tradition of readability, accessibility, and usefulness to students, with a team at McGraw-Hill supporting the author in this endeavor.\n",
      "=== LLM Response ===\n",
      "The book \"Human Physiology\" by Fox is currently in its sixteenth edition and is published by McGraw Hill LLC. It aims to help students understand the concepts of human physiology in a readable, current, and student-oriented manner. The sixteenth edition has been updated with contributions from Krista Rompolski, Ph.D., from Moravian College, who significantly revised chapters 8 and 18. The book maintains its tradition of readability, accessibility, and usefulness to students, with a team at McGraw-Hill supporting the author in this endeavor.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11763ad4-c58e-4a1c-aec6-fbf4206e5e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT framework are Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and expertise that contribute to the efficient breakdown and execution of complex tasks within the multi-agent system. The Product Manager generates quadrant charts for applications, the Architect devises technical specifications and designs system architecture, the Project Manager assigns tasks to Engineers, the Engineer completes development tasks, and the QA Engineer ensures high-quality software through unit test code generation.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"communication between agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The communication between agent roles in MetaGPT is structured and efficient, with each agent having a specific role and expertise. The communication protocol involves structured interfaces and a publish-subscribe mechanism, allowing agents to exchange information transparently through a shared message pool. This structured communication approach enhances efficiency and reduces information overload, ensuring that agents receive only relevant task-related information based on their role profiles. The Architect agent devises technical specifications, which are then passed on to the Project Manager for task allocation and execution. The Engineer agent utilizes the provided file structure and function definitions to complete development tasks, while the QA Engineer generates unit test code based on the output from the Engineer and reviews it for quality assurance. This structured communication flow ensures that each agent performs their specific role effectively, leading to the successful development of high-quality software projects in MetaGPT.\n",
      "=== LLM Response ===\n",
      "In MetaGPT, there are five agent roles: Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities contributing to task execution within the multi-agent system. The communication between these roles is structured and efficient, facilitated by interfaces and a publish-subscribe mechanism. The Architect devises technical specifications, which are then passed to the Project Manager for task allocation. The Engineer completes development tasks using the provided structure, and the QA Engineer ensures software quality through unit test code generation. This structured communication flow ensures effective task execution and high-quality software development in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9f465-3f2e-4c2f-bddb-d10ac794d6d1",
   "metadata": {},
   "source": [
    "When you run a multi-step query like this. You want to make sure that you're actually able to trace the sources. \n",
    "\n",
    "So luckily, similar to previous lessons, can look at `response.source_nodes`. Take a look at the content of these nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f38403f6-c9f9-44b6-8083-63975f9d11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: data/pdfs/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-06-07\n",
      "last_modified_date: 2024-06-07\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e0d54-4d18-4893-b944-4dc44a66028b",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/full_agent_reasoning_loop.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82a88a-d5ca-431c-a73b-ef1fcf13e704",
   "metadata": {},
   "source": [
    "Calling `agent.query()` allows you to query the agent in a one-off manner, but does not preserve state. \n",
    "\n",
    "So now let's **try maintaining conversation history over time**.\n",
    "\n",
    "The agent is able to maintain chats in a conversational memory buffer.\n",
    "\n",
    "The memory module can be customized - by default is a flat list of items that's a rolling buffer depending on the size of the context window of the LLM.\n",
    "\n",
    "Therefore, when the agent decides to use a tool, it not only uses the current chat, but also previously the previous conversation history to take the next step/perform next action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c8e4bf9-2ddd-4596-b5d8-e7abcb57d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation datasets in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets in MetaGPT include HumanEval and MBPP benchmarks, which assess functional accuracy and performance in code generation tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval and MBPP benchmarks. These datasets are utilized to assess functional accuracy and performance in code generation tasks within the MetaGPT framework.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5570f-013e-4618-94b9-a8cd13f756fc",
   "metadata": {},
   "source": [
    "Now let's ask a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a07d2642-29ee-4ca2-b3fe-7a72f4711522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the results over one of the above datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"results over HumanEval dataset\", \"page_numbers\": [\"7\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieved a Pass rate of 85.9% and 87.7% over the HumanEval dataset.\n",
      "=== LLM Response ===\n",
      "MetaGPT achieved a Pass rate of 85.9% and 87.7% over the HumanEval dataset.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me the results over one of the above datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa4542-1098-42ef-be06-1121406b4dbe",
   "metadata": {},
   "source": [
    "### Low-level Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d0e81-0b1b-4b20-b657-6d66deed28c3",
   "metadata": {},
   "source": [
    "Just provided a nice, high-level interface for interacting with an agent.\n",
    "\n",
    "Next section will show you capabilities that let you step through and control the agent in a much more granular fashion. Not only allows you to create a higher level research assistant over your RAG pipelines, but also debug/control it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7a90c-316b-4a47-978f-51b0534e078a",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/agent_control.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fc0cf-8522-4ab1-9ed1-6039c5733749",
   "metadata": {},
   "source": [
    "Having this low-level agent interace is powerful for two main reasons:\n",
    "1. **Allows developers of agents to have greater transparency/visibility into what's actually going on under the hood**, especially if the agent isn't working the first time around, can go in, trace thru execution of the agent, see where it is failing, try out different inputs to if that actually modifies the agent execution into a correct response\n",
    "\n",
    "2. **Enable richer UX's when building a product experience around this core agentic capability**. For instance, let's say you want to listen to human feedback in the middle of agent execution, as opposed to only after the agent execution is complete for a given task. Then can imagine creating some sort of async queue, where you're able to listen to inputs from humans throughout the middle of agent execution and if human input comes in, can actually interrupt and modify the execution of an agent as it's going on thru a larger task, as opposed to having to wait until the agent's task is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c5ef3d1-c8ed-4a14-9def-e46676a8e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef67ad-4090-43cd-b65a-94b4f55ada28",
   "metadata": {},
   "source": [
    "Let's **start using the low-level API**. \n",
    "\n",
    "1. We'll first ***create a task object*** from the user query\n",
    "\n",
    "2. And then we'll start running thru steps or ***even interjecting our own***. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18f748-cd26-4b44-b653-234c2792e901",
   "metadata": {},
   "source": [
    "Let's try executing a single step of this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebcb8752-9c6d-4763-9500-466b3933fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "efcf16cb-52c7-4962-a3d7-42d45f31a046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a7707354-71f2-462a-a423-e992f9d7c0af'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce1f4e-8367-46cc-879e-570d4cdec787",
   "metadata": {},
   "source": [
    "We created a task for this agnet; this will return a **task object** which ***contains the input*** as well as ***additional state*** in the task object.\n",
    "\n",
    "Now let's try **executing a single step of this task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4161123-e479-4b48-b8cc-18ba1e346f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and tasks assigned to them, contributing to the overall software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the technical specifications and system architecture, the Project Manager breaks down tasks and allocates them, the Engineer implements the code based on specifications, and the QA Engineer ensures code quality through testing and bug fixing. These roles work together in a structured workflow to efficiently develop software solutions.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe3bee-21fe-478d-a351-20f2ac392fc2",
   "metadata": {},
   "source": [
    "The agent executes a step of this task through the task ID and give you back a step output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d31488-18d3-439b-aaae-a4e2cf109f2f",
   "metadata": {},
   "source": [
    "It calls the summary tool with the input: \"agent roles in MetaGPT\", which is the very first part of this question. And then it stops there.\n",
    "\n",
    "When we inspect the logs, and the output of the agent, we see that the first part was actually executed. So we call `agent.get_completed_steps()` with the *task_id*, and we're able to look at the num completed for the task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "93dae43e-80a5-4fb3-94e4-7c36ab2f6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task a7707354-71f2-462a-a423-e992f9d7c0af: 1\n",
      "The agent roles in MetaGPT include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and tasks assigned to them, contributing to the overall software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the technical specifications and system architecture, the Project Manager breaks down tasks and allocates them, the Engineer implements the code based on specifications, and the QA Engineer ensures code quality through testing and bug fixing. These roles work together in a structured workflow to efficiently develop software solutions.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd294e27-be20-4723-9067-71f0f1cd9b09",
   "metadata": {},
   "source": [
    "We see that one step has been completed and this is the current output so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19e055-bbb4-4fe4-a0a0-e30041251bf4",
   "metadata": {},
   "source": [
    "We can also take a look at any upcoming steps for the agent thru `agent.get_upcoming_steps()` and passing the *task_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afb92c91-b5cf-43fc-97ab-30a0781384ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num upcoming steps for task a7707354-71f2-462a-a423-e992f9d7c0af: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='a7707354-71f2-462a-a423-e992f9d7c0af', step_id='4c8c291a-1009-43ed-a9b0-de92e01de8f4', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9664343-4359-421c-8e0e-65224a25539d",
   "metadata": {},
   "source": [
    "We see it's also 1, and we're able to look at a `TaskStep` object with a *task id*, as well as an existing input. This input is currently `None` because the way the agent works is it actually just **autogenerates the action from the conversation history**, and doesn't need to generate an additional external input.\n",
    "\n",
    "The nice thing about this debugging interface is that if you wanted to pause execution now, you can. You can take the intermediate results without completing the agent flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f55ced-166f-4251-8fcf-23ab40a92afc",
   "metadata": {},
   "source": [
    "Let's keep going and run the next two steps and actually try injecting user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c32860e-b402-4ead-983d-f02580b26ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What about how agents share information?\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"how agents share information in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This system allows agents to exchange structured messages directly and subscribe to relevant information based on their roles. The shared message pool facilitates the exchange of task-related information efficiently, while the publish-subscribe mechanism ensures that agents receive only the necessary information, thus enhancing communication effectiveness within the multi-agent framework.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"What about how agents share information?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba11cb2-623a-4ca5-a1c8-20b9116f14e9",
   "metadata": {},
   "source": [
    "Not part of the original task query, but by injecting this, can actually modify agent execution to give you back the result that you want.\n",
    "\n",
    "We'll see that we added the *user message* to **memory**. Next call is \"how agents share information in MetaGPT\".\n",
    "\n",
    "We see here it's able to give back a response. Overall task is complete, just need to run 1 final step to synthesize the answer, and to double check that this output is the last step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f8bf9bd-7061-4a2d-b91f-8d69b19300f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "In MetaGPT, agents share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This system allows agents to exchange structured messages directly and subscribe to relevant information based on their roles. The shared message pool facilitates the exchange of task-related information efficiently, while the publish-subscribe mechanism ensures that agents receive only the necessary information, thus enhancing communication effectiveness within the multi-agent framework.\n"
     ]
    }
   ],
   "source": [
    "# one final step to synthesize the answer\n",
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1de7fb58-4b81-4299-825d-a93e9697d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520b11c-bd7f-4e28-b3bd-cdc5ed829782",
   "metadata": {},
   "source": [
    "We get back the answer, this is the last step. To translate this into an **agent response**, similar to what we've seen in some of the previous notebook cells, then all you have you do is call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "272aca1c-3e9e-4dd7-8366-674466febe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "486ec7ec-92d5-4b7b-a9ec-c9418284bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In MetaGPT, agents share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This system allows agents to exchange structured messages directly and subscribe to relevant information based on their roles. The shared message pool facilitates the exchange of task-related information efficiently, while the publish-subscribe mechanism ensures that agents receive only the necessary information, thus enhancing communication effectiveness within the multi-agent framework.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a680af-27ab-4a19-9e2f-c17da9ef5911",
   "metadata": {},
   "source": [
    "That's it! You've learned about: The high-level interface for an agent, as well as a low-level debugging interface.\n",
    "\n",
    "In the next lesson, we'll learn how to build an agent over multiple documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20087676-1476-4b44-811c-6b0c17a5ae81",
   "metadata": {},
   "source": [
    "## Building a Multi-Document Agent\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f08e45-d909-4458-af65-a216701afa9c",
   "metadata": {},
   "source": [
    "### Three Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a6590-8897-4732-b43d-9ee758d11370",
   "metadata": {},
   "source": [
    "In previous lesson, we:\n",
    "* Built an agent that can reason over a single document.\n",
    "\n",
    "* And answer complex questions over it while maintaining memory.\n",
    "\n",
    "* In this lesson **we will learn how to extend that agent to handle multiple documents in increasingly degrees of complexity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2375159-bedf-4703-b888-692253806427",
   "metadata": {},
   "source": [
    "We will start with a 3 document use-case, then we will expand to a 11 document use-case:\n",
    "\n",
    "\n",
    "\n",
    "![Multi Doc Agent](imgs/multi_doc_agent.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1a450-8569-48d2-8885-cc5dd3f59d73",
   "metadata": {},
   "source": [
    "Let's setup our OpenAI key and import nest asyncio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a24b741-cd61-4416-a2bd-a5c1fd46bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Accessing the environment variable\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded properly\n",
    "if openai_api_key is not None:\n",
    "    print(\"OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load OPENAI_API_KEY. Please check if it is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee575fe2-dc00-4146-8095-f1a6d34dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5d3c9-e461-4b4a-a4dc-96bc19eb23c9",
   "metadata": {},
   "source": [
    "First task is to **setup our function calling agent over three papers.**\n",
    "\n",
    "We do this by **combining the vector summary tools for each document into a list, and passing it to the agent so that the agent actually has 6 tools in total**.\n",
    "\n",
    "Let's download these 3 papers:\n",
    "\n",
    "1. METAGPT: META PROGRAMMING FOR A\n",
    "MULTI-AGENT COLLABORATIVE FRAMEWORK\n",
    "2. LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS\n",
    "3. SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\n",
    "CRITIQUE THROUGH SELF-REFLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec8e7a1-fd13-4293-a649-540410ac48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your base directory\n",
    "base_dir = 'data/pdfs'\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\"\n",
    "]\n",
    "\n",
    "# Generate the full paths for each paper\n",
    "papers = [os.path.join(base_dir, paper) for paper in papers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0d41b-010f-45cd-9444-3005fef0afa6",
   "metadata": {},
   "source": [
    "Next we will convert each paper into a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a3ec9fb-851e-44c1-abf9-faf1236b476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: data/pdfs/metagpt.pdf\n",
      "Getting tools for paper: data/pdfs/longlora.pdf\n",
      "Getting tools for paper: data/pdfs/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "# automatically builds a vector index tool + summary index tool \n",
    "# over a given paper\n",
    "\n",
    "# Vector tool - performs vector search\n",
    "# Summarization tool - performs summarization across entire document\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f71f94-112a-44d4-893b-16bcf74931b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(paper_to_tools_dict[papers[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1839d-e9b4-460c-8071-a0ee9898ffb8",
   "metadata": {},
   "source": [
    "So for each paper we get back:\n",
    "1. Vector tool\n",
    "2. Summary tool\n",
    "\n",
    "We put it into this overall dictionary, mapping each paper name to the vector tool and summary tool.\n",
    "\n",
    "Next we'll simply get these tools in a flat list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "556b1d97-0ad1-45cb-80df-707d3dff6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641259a2-598b-4a65-a582-2bb60f20135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# define LLM of choice\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d73c66-8ef6-470c-8533-fd75e9b7953f",
   "metadata": {},
   "source": [
    "Let's look at the tools being passed to the agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8c0d2-0ee8-40b6-bcbb-1c9b72e918ba",
   "metadata": {},
   "source": [
    "![Three Doc Agent](imgs/three_document_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9581b-0ae2-41cd-8f9a-d2f57ab94c25",
   "metadata": {},
   "source": [
    "6 tools: We have **three papers**, **two tools** for each paper (vector and summary tool).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d6b3c17-eedb-40a3-804d-a79db1c187fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904adf5-c3d6-4241-890e-39ce4bae7911",
   "metadata": {},
   "source": [
    "Next step is to construct our overall **agent worker**. This agent worker includes the six tools as well as the LLM that we pass to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb3f69e4-0bc0-4864-ae0e-5dfa34d4030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b396d-99e4-4ae7-8099-b97d48e9902c",
   "metadata": {},
   "source": [
    "Now able to **ask questions across these three documents, or over a single document**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731eb801-9e5b-4342-9ba6-89b203d24f79",
   "metadata": {},
   "source": [
    "![Three Doc Agent Query](imgs/three_document_agent_query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b57ca0b-c710-484e-802e-989f26e21d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments mentioned in the context is the PG19 test split.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results include the perplexity values for different models and baselines on the proof-pile (Azerbayev et al., 2022) and PG19 datasets. The models achieve better perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. The perplexity decreases as the context size increases, with improvements observed when increasing the context window size. Additionally, the maximum context lengths that can be fine-tuned on a single 8 × A100 machine are extended for different models, showing promising results on extremely large settings. However, there is some perplexity degradation on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. The evaluation results include perplexity values for different models and baselines on the proof-pile (Azerbayev et al., 2022) and PG19 datasets. The models achieve better perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. Perplexity decreases as the context size increases, with improvements observed when increasing the context window size. The maximum context lengths that can be fine-tuned on a single 8 × A100 machine are extended for different models, showing promising results on extremely large settings. However, there is some perplexity degradation on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2b8d6-475c-43f2-bc39-70eef5efa7b6",
   "metadata": {},
   "source": [
    "We get back the answer, PG19 test split was one of the eval datasets used, and then we're able to workout the eval results for LongLoRA models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55791ef6-94da-411d-bd9b-4012976ee15a",
   "metadata": {},
   "source": [
    "The next question we can ask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce76739c-9c83-450e-95ba-c2b95becf74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework designed to improve the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It enables a single arbitrary LM to retrieve, generate, and critique text passages and its own outputs using reflection tokens. This approach allows the model to adjust its behavior during inference to better meet the requirements of different tasks. Experimental results have shown that Self-RAG surpasses other language models and retrieval-augmented models in various tasks, showcasing its effectiveness in enhancing performance, factuality, and citation accuracy.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models while minimizing GPU memory cost and training time compared to standard full fine-tuning. It introduces S2-Attn to approximate the standard self-attention pattern during training. The method retains the original attention architecture during inference, making existing infrastructure and optimization reusable. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by incorporating trainable normalization and embedding layers. The approach has been successful in extending models like Llama2 7B to 100k context length and 70B models to 32k on a single 8×A100 machine.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework designed to enhance the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It allows a single language model to retrieve, generate, and critique text passages and its own outputs using reflection tokens, enabling the model to adjust its behavior during inference for different tasks. Experimental results demonstrate that Self-RAG outperforms other language models and retrieval-augmented models in various tasks, showcasing its effectiveness in improving performance, factuality, and citation accuracy.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models while minimizing GPU memory cost and training time compared to standard full fine-tuning. It introduces S2-Attn to approximate the standard self-attention pattern during training, retaining the original attention architecture during inference for reusability of existing infrastructure and optimization. LongLoRA bridges the gap between LoRA and full fine-tuning by incorporating trainable normalization and embedding layers, successfully extending models like Llama2 7B to 100k context length and 70B models to 32k on a single 8×A100 machine.\n",
      "Self-RAG is a framework designed to enhance the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It allows a single language model to retrieve, generate, and critique text passages and its own outputs using reflection tokens, enabling the model to adjust its behavior during inference for different tasks. Experimental results demonstrate that Self-RAG outperforms other language models and retrieval-augmented models in various tasks, showcasing its effectiveness in improving performance, factuality, and citation accuracy.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models while minimizing GPU memory cost and training time compared to standard full fine-tuning. It introduces S2-Attn to approximate the standard self-attention pattern during training, retaining the original attention architecture during inference for reusability of existing infrastructure and optimization. LongLoRA bridges the gap between LoRA and full fine-tuning by incorporating trainable normalization and embedding layers, successfully extending models like Llama2 7B to 100k context length and 70B models to 32k on a single 8×A100 machine.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977a6b3-0eb0-4bde-886d-f082e5ee1506",
   "metadata": {},
   "source": [
    "Try out some queries on your own by trying any combination of these papers and **ask for both summaries as well as specific information within the papers**, to see whether or not the agent is able to reason about the summary and vector tools within each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7f3ca-b65b-429f-82ef-e2fb47c5a7ac",
   "metadata": {},
   "source": [
    "### Inspecting a Single Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae27468-16e5-48d5-81c6-761dcc6175ad",
   "metadata": {},
   "source": [
    "Getting LlamaIndex Tool metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe00a9a1-e140-4205-80d2-f4853faa7351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='vector_tool_metagpt(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n\\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n\\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n\\n        ', name='vector_tool_metagpt', fn_schema=<class 'pydantic.v1.main.vector_tool_metagpt'>, return_direct=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool = paper_to_tools_dict[papers[0]][0]\n",
    "\n",
    "tool.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef2042-1e3a-4d6a-bb70-153810eba89f",
   "metadata": {},
   "source": [
    "Using *python* `inspect` module to look at the function behind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c591db1-c766-49c3-b653-580bdc4bfa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: vector_query\n",
      "Function Signature: (query: str, page_numbers: Optional[List[str]] = None) -> str\n",
      "Tool Name: vector_tool_metagpt\n",
      "Tool Description: vector_tool_metagpt(query: str, page_numbers: Optional[List[str]] = None) -> str\n",
      "Use to answer questions over a given paper.\n",
      "\n",
      "        Useful if you have specific questions over the paper.\n",
      "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
      "\n",
      "        Args:\n",
      "            query (str): the string query to be embedded.\n",
      "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
      "                if we want to perform a vector search\n",
      "                over all pages. Otherwise, filter by the set of specified pages.\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# Assuming paper_to_tools_dict and papers are defined\n",
    "\n",
    "# Access the specific tool object\n",
    "tool = paper_to_tools_dict[papers[0]][0]\n",
    "\n",
    "# Access the function wrapped by the tool\n",
    "function = tool.fn\n",
    "function_name = function.__name__  # Get the name of the function\n",
    "function_signature = inspect.signature(function)  # Get the signature of the function\n",
    "\n",
    "# Access the metadata\n",
    "metadata = tool.metadata\n",
    "tool_name = getattr(metadata, 'name', 'No name provided')  # Access the name attribute from metadata\n",
    "tool_description = getattr(metadata, 'description', 'No description provided')  # Access the description attribute from metadata\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Signature:\", function_signature)\n",
    "print(\"Tool Name:\", tool_name)\n",
    "print(\"Tool Description:\", tool_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3faa7a8-e691-46d1-95ae-44e608ed173b",
   "metadata": {},
   "source": [
    "*Note*: `inspect` module is a standard Python library that provides several useful functions to help get information about live objects such as modules, classes, methods, functions, tracebacks, frame objects, and code object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb81aa27-f5ed-46fe-9082-689be61bf09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature: (x: int, y: int) -> int\n",
      "Name: x, Type: <class 'int'>, Default: <class 'inspect._empty'>\n",
      "Name: y, Type: <class 'int'>, Default: <class 'inspect._empty'>\n",
      "Return type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# how `inspect` works in python\n",
    "import inspect\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers and returns the result.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# Retrieve the signature of the 'add' function\n",
    "signature = inspect.signature(add)\n",
    "print(\"Signature:\", signature)\n",
    "\n",
    "# Print detailed parameter information\n",
    "for name, param in signature.parameters.items():\n",
    "    print(f\"Name: {name}, Type: {param.annotation}, Default: {param.default}\")\n",
    "\n",
    "# You can also access the return annotation\n",
    "print(\"Return type:\", signature.return_annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801d940-0d59-4196-b0b5-d0ad5b8589f7",
   "metadata": {},
   "source": [
    "This output shows you how to call the function and what arguments it expects. The `inspect` module is very useful for developers who need to understand or debug code, especially when working with complex and dynamically created objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692c54e-9a41-4ab5-b4a8-32b70d3e2233",
   "metadata": {},
   "source": [
    "### Eleven Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddcef0-bdd3-4bd4-a95e-a2e03a07d4f8",
   "metadata": {},
   "source": [
    "Let's expand into a more advanced use-case: 11 papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "986a5811-3209-4da5-88e9-e47f650e2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]\n",
    "\n",
    "# Generate the full paths for each paper\n",
    "papers = [os.path.join(base_dir, paper) for paper in papers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff495c3f-d2ab-4591-a345-1cbf112712d5",
   "metadata": {},
   "source": [
    "Similar to previous section, we will now build a dictionary mapping each paper to its vector/summary tool (can take a little bit of time, since we need to process, index, and embed 11 documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe5b99ad-4344-4e38-8116-b8a87c597417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: data/pdfs/metagpt.pdf\n",
      "Getting tools for paper: data/pdfs/longlora.pdf\n",
      "Getting tools for paper: data/pdfs/loftq.pdf\n",
      "Getting tools for paper: data/pdfs/swebench.pdf\n",
      "Getting tools for paper: data/pdfs/selfrag.pdf\n",
      "Getting tools for paper: data/pdfs/zipformer.pdf\n",
      "Getting tools for paper: data/pdfs/values.pdf\n",
      "Getting tools for paper: data/pdfs/finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: data/pdfs/knowledge_card.pdf\n",
      "Getting tools for paper: data/pdfs/metra.pdf\n",
      "Getting tools for paper: data/pdfs/vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e61f11f6-da9e-4cc3-a5da-03eeeeec4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse these tools into a flat list\n",
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef1e5c-39f6-4934-a094-835a998fe6b9",
   "metadata": {},
   "source": [
    "Here we will **need a slightly more advanced agent and tool architecture**.\n",
    "\n",
    "Issue is that let's say we try to index all 11 papers, which now includes 20 tools. Or we try to index a 100 paper, 1000 papers, etc.\n",
    "\n",
    "Even though LLM context windows are getting longer - stuffing too many tool selections into the LLM prompt leads to the following issues:\n",
    "\n",
    "1. **Tools may not all fit in the prompt**, especially if your number of documents are big, and you're modeling each document as a separate tool or a set of tools.\n",
    "2. **Cost and latency will spike**, because you're increasing the number of tokens in your prompt.\n",
    "3. **LLM can get confused**, may fail to pick the right tool when the number of choices is too large.\n",
    "\n",
    "A *solution* is **When the user asks a query, we actually peform Retrieval Augmentation, but not on the level of text, but actually on the level of tools.** \n",
    "\n",
    "1. We first retrieve a small set of relevant tools, and then feed the relevant tools to the agent reasoning prompt, instead of all the tools. This retrieval process is similar to the retrieval process used in RAG. At its simplest can just be a *top-k* vector search. But of course - can advance all the retrieval techniques you want to filter out the relevant set of results.\n",
    "\n",
    "The LlamaIndex agents let you plug in a **tool retriever**, that allows you to accomplish exactly this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1b6f2-8ec7-4817-a437-1c67cc62f330",
   "metadata": {},
   "source": [
    "![Tool RAG 1](imgs/tool_rag_1.png)\n",
    "![Tool RAG 2](imgs/tool_rag_2.png)\n",
    "![Tool RAG 3](imgs/tool_rag_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593406ca-e610-4d33-ae9a-13b9734352a8",
   "metadata": {},
   "source": [
    "Let's show you how to get this done. First we will want to **index the tools**.\n",
    "\n",
    "LlamaIndex has extensive indexing capabilities over general text objects, but since these tools are python objects, we need a way to convert and serialize these objects into a string representation and back.\n",
    "\n",
    "This is all thru the `ObjectIndex` abstraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe994c78-86b5-4ac5-8295-6eb5bdae70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex # standard interface for indexing text \n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "# directly plug in these python tools as input into the index\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143b7fd-34f0-47bb-b99e-095f1da7ab05",
   "metadata": {},
   "source": [
    "We wrap the standard `VectorStoreIndex` with `ObjectIndex`. To construct an object index, we directly plug-in these python tools as input to the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb804e4-82db-4d27-814c-48463395cc4c",
   "metadata": {},
   "source": [
    "You can retrieve from an object index using **object retriever**. This will call the underlying retriever from the index, and return the output directly as objects. In this case, it will be tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a49f607-9a30-4465-8fef-aef9571fb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb7aaa-60bc-40c1-be6e-781e1189f73f",
   "metadata": {},
   "source": [
    "Now let's walk thru a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e610ca50-1660-4afd-94cd-c74f99c31f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fde6c236-332f-4cdf-a454-c33eb87f2db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first tool in this list\n",
    "\n",
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1755f-0eac-4121-b969-26912796d9da",
   "metadata": {},
   "source": [
    "We directly received a set of tools, and the first one is the **summary tool for metagpt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40bf14a9-1de8-447e-8cd0-ac57b94e2f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metra', name='summary_tool_metra', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second tool in this list\n",
    "\n",
    "tools[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7171e483-f807-4ecf-a767-4a4db65423d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third tool in this list\n",
    "\n",
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc57801-1bd3-400a-b914-021335d73410",
   "metadata": {},
   "source": [
    "**Of course, the quality of retriever is dependant on your embedding model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e88761-a88d-4329-9e61-2adafee30d4e",
   "metadata": {},
   "source": [
    "Now we are **ready to setup our function calling agent**. \n",
    "\n",
    "Setup is pretty similar to the setup in the last lesson, however as an additional feature, we show that you can add a system prompt to the agent if you want \n",
    "\n",
    "***(Optional)*** **Helps provide additional guidance if you want to prompt the agent to output things in a certain way.**\n",
    "\n",
    "**Useful if you want it \\[agent\\] to take into account certain factors when it reasons over these tools.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "471c4807-01be-494c-917a-3e7e151c346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of system prompt\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8902c1e-1e39-44c9-b729-312321cce3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. The dataset includes tasks such as creating games like Snake, Brick Breaker, 2048, Flappy Bird, and Tank Battle, as well as tasks involving Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game implementation, weather dashboard creation, and more. The dataset prompts users to develop various software applications and systems, providing a range of challenges to assess MetaGPT's performance in generating executable code for different types of tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, such as being merged, resolving one or more issues, and introducing new tests. Each task instance includes the codebase, problem statement aggregated from related issues, a test patch, and a gold patch. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. It includes tasks such as creating games like Snake, Brick Breaker, 2048, Flappy Bird, and Tank Battle, as well as tasks involving Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game implementation, weather dashboard creation, and more. The dataset prompts users to develop various software applications and systems, providing a range of challenges to assess MetaGPT's performance in generating executable code for different types of tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, such as being merged, resolving one or more issues, and introducing new tests. Each task instance includes the codebase, problem statement aggregated from related issues, a test patch, and a gold patch. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models.\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. It includes tasks such as creating games like Snake, Brick Breaker, 2048, Flappy Bird, and Tank Battle, as well as tasks involving Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game implementation, weather dashboard creation, and more. The dataset prompts users to develop various software applications and systems, providing a range of challenges to assess MetaGPT's performance in generating executable code for different types of tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, such as being merged, resolving one or more issues, and introducing new tests. Each task instance includes the codebase, problem statement aggregated from related issues, a test patch, and a gold patch. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models.\n"
     ]
    }
   ],
   "source": [
    "# comparison queries\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d605f5-64c9-408a-9a5b-46fc586687ea",
   "metadata": {},
   "source": [
    "### Compare and Contrast Wwo LoRa papers\n",
    "\n",
    "This is the final example. Let's compare and contrast the two LoRA papers: `LongLoRA` and `LoftQ`. and analyze the approach in each paper first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "676f3451-3061-4023-af03-219fef1598d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning method that extends the context length of large language models while minimizing GPU memory cost and training time compared to standard full fine-tuning. It introduces S2-Attn to approximate the standard self-attention pattern during training, allowing for improved efficiency and performance in extending the context length of language models.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a quantization framework designed for Large Language Models (LLMs) that simultaneously applies quantization and low-rank approximation to the original high-precision pre-trained weights. This approach aims to provide an initialization for subsequent Low-Rank Adaptation (LoRA) fine-tuning, leading to improved performance in downstream tasks. LoftQ has been shown to outperform existing quantization methods, particularly in challenging low-bit quantization scenarios, such as the 2-bit level. LoftQ is implemented based on the publicly available Huggingface code-base and utilizes specific hyperparameters for different tasks, aiming to reduce memory usage during both training and storage compared to other methods like pruning, showcasing improved performance on models like DeBERTaV3-base.\n",
      "=== LLM Response ===\n",
      "LongLoRA is an efficient fine-tuning method that extends the context length of large language models while minimizing GPU memory cost and training time compared to standard full fine-tuning. It introduces S2-Attn to approximate the standard self-attention pattern during training, allowing for improved efficiency and performance in extending the context length of language models.\n",
      "\n",
      "LoftQ, on the other hand, is a quantization framework designed for Large Language Models (LLMs) that simultaneously applies quantization and low-rank approximation to the original high-precision pre-trained weights. This approach aims to provide an initialization for subsequent Low-Rank Adaptation (LoRA) fine-tuning, leading to improved performance in downstream tasks. LoftQ has been shown to outperform existing quantization methods, particularly in challenging low-bit quantization scenarios, such as the 2-bit level. LoftQ is implemented based on the publicly available Huggingface code-base and utilizes specific hyperparameters for different tasks, aiming to reduce memory usage during both training and storage compared to other methods like pruning, showcasing improved performance on models like DeBERTaV3-base.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0a2b1-7957-4d03-86f7-f6fb1bbe75aa",
   "metadata": {},
   "source": [
    "First step it takes, is it takes this input task and actually retrieves the set of input tools that help it fulfill its task. Thru the `object retriever` the **hope it that it retrieves LongLoRA and loftq query tools in order help it fulfill its response**.\n",
    "\n",
    "If we take a look at the intermediate outputs of the agent, we see **it is able to have access to relevant tools** from LongLoRA and also loftq.\n",
    "\n",
    "First calls `summary_tool_longlora with args: {\"input\": \"LongLoRA\"}` gets back summary.\n",
    "\n",
    "Then calls `summary_tool_loftq with args: {\"input\": \"LoftQ\"}` gets back summary.\n",
    "\n",
    "Final LLM response is able to compare these two approaches by comparing the responses from these two tools, and combining it to synthesize and answer that satisfies the user query.\n",
    "\n",
    "Can now build agents over multiple documents. Enabling us to build more general, complex, context-augmented research assistants that can answer complex questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779aa3a-e2cd-4b28-9e7c-964b5c216365",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***\n",
    "\n",
    "Complete!\n",
    "\n",
    "We learned about:\n",
    "\n",
    "* Agentic RAG: starting from building a router agent, to tool calling, to building your own agent that can reason not just over a single document, but over multiple documents.\n",
    "\n",
    "* If you want to build custom agents, refer to these resources. \n",
    "\n",
    "* **Custom agents**: https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent/\n",
    "\n",
    "\n",
    "* **Community-built Agents (LlamaHub)**: https://llamahub.ai/?tab=agent\n",
    "\n",
    "\n",
    "* **Advaned document parsing with LlamaParse**: https://cloud.llamaindex.ai\n",
    "\n",
    "Build cool stuff with **agentic RAG**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb73cc8-f4d0-4f06-a772-16643b57e72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
