{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1e56fa-162b-490c-9379-5f402b7a43ad",
   "metadata": {},
   "source": [
    "# Building Agentic RAG with Llamaindex\n",
    "\n",
    "[deeplearning.ai course link here](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/1/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b6d64-3b0c-4b48-9090-57707a7d5751",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***\n",
    "\n",
    "* Jerry Liu: co-founder and CEO of LlamaIndex and instructor of course.\n",
    "\n",
    "* Will learn about agentic RAGs, a framework to help you build research agents, capable of doing reasoning, decision making over your data.\n",
    "\n",
    "* Standard RAG pipeline is mostly good for simpler questions over a small set of documents, works by retrieving some context and sticking to an LLM prompt and calling it a single time to get a response.\n",
    "\n",
    "* We will build an autonomous research agent. Will learn **a progression of reasoning ingredients** to build a full agent:\n",
    "\n",
    "    * **Routing**: We add decision making to route requests to multiple tools\n",
    "    * **Tool-use**: Create interface for agents to select a tool and generate the right arguments for that tool.\n",
    "    * **Multi-step reasoning**: Use an LLM to perform reasoning with a range of tools for retaining memory throughout that process.\n",
    "\n",
    "\n",
    "* Let user optionally inject guidance at intermediate steps (e.g. guiding when searching a document). Just like an experienced manager giving a junior employee a nudge to consider a new piece of info to achieve much better performance.\n",
    "\n",
    "* **First lesson**: Build a router over a single document that can handle question-answering and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48a148-9de9-47d0-b195-c203bf470fd6",
   "metadata": {},
   "source": [
    "## Router Query Engine\n",
    "***\n",
    "\n",
    "Simplest form of agentic RAG. Given a query, router picks one of several query entrants (ways) to execute a query. Will build a single router for a single document to do question-answering and summarization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e476d03-4b96-4c30-9d7d-5010cea1803c",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/router_engine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dc8ce-8770-45f4-b27c-ab88ff4725ae",
   "metadata": {},
   "source": [
    "Load OpenAI API Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069da77b-46b8-4750-bb6d-983e089e202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Accessing the environment variable\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded properly\n",
    "if openai_api_key is not None:\n",
    "    print(\"OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load OPENAI_API_KEY. Please check if it is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32535534-bf44-438d-b768-165db434b043",
   "metadata": {},
   "source": [
    "Jupyter runs an event loop behind the scenes, and a lot of our modules use async; to make async play nicely with Jupyter Notebooks we need this `nest_asyncio` package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b478cc5-fd0d-4b11-8cea-5e4e62835f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446489c-7f2a-41c7-a840-eccb64c8e18e",
   "metadata": {},
   "source": [
    "Next is to load in a sample PDF document. Read PDF into a **parsed document representation**.\n",
    "\n",
    "We will use the `SimpleDirectoryReader` module and *LlamaIndex* to read in this PDF into a parsed document representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6e96cb-e460-4e49-adcb-4716717d7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full path to the file is: data/pdfs/P1-mher-arabian.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Define your base directory\n",
    "base_dir = 'data/pdfs'\n",
    "\n",
    "# Define your file name\n",
    "file_name = 'P1-mher-arabian.pdf'\n",
    "\n",
    "file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "print(\"The full path to the file is:\", file_path)\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[file_path]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb666816-f131-4dd2-8db6-02e6189cfc04",
   "metadata": {},
   "source": [
    "Next we'll split documents into even size chunks using the `SentenceSplitter`, we'll split on the order of sentences. We set the chunk size to 1024. \n",
    "\n",
    "We call `splitter.get_nodes_from_documents()` to split these documents into **nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bc0e64-6325-4c56-8857-c7a5ebdefe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3813e39f-2d18-4875-a5c7-834dba754725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of nodes object is: <class 'llama_index.core.schema.TextNode'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of nodes object is: {type(nodes[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8293f63-5300-4507-aea5-8efccef5ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the nodes list is: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of the nodes list is: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af2f28-8012-4662-b552-142c7edef8ce",
   "metadata": {},
   "source": [
    "This next step allows us to define an LLM and embedding model. Can do this by specifying a global config setting where you specify the LLM and embedding model that you want to inject as part of the global config.\n",
    "\n",
    "By default we use 3.5 turbo and text embedding ada-002 in this course. This allows you to have the groundwork to inject your own LLMs and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556f2a82-0218-4ca1-bf13-f57fddd3b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de946115-8407-465f-b3b8-cc6f764c7ef8",
   "metadata": {},
   "source": [
    "Now we are ready to start building some indexes. Here we define two indexes, over these nodes:\n",
    "\n",
    "* Summary Index\n",
    "* Vector Index\n",
    "\n",
    "Can think of an index as a set of of metadata over our data.  You can query an index, and different indexes will have different retrieval behaviors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90e416-c6b9-47b9-a51a-03397897e38e",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/vector_index_vs_summary_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745db65b-e559-44bb-b8e1-c931be4ad909",
   "metadata": {},
   "source": [
    "### Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52baf2-0f49-4764-a6b0-fb43535c13c7",
   "metadata": {},
   "source": [
    "A **vector index** indexes nodes via text embedings and is a core abstraction in LlamaIndex and in any sort of RAG system. \n",
    "\n",
    "Querying a vector index will return the most similar nodes by embedding similarity.\n",
    "\n",
    "\n",
    "![Router Engine](imgs/vector_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586763b2-8c6c-4572-8042-31e3aa175384",
   "metadata": {},
   "source": [
    "### Summary Index\n",
    "\n",
    "* Very simple index -  querying it will return all the nodes currently in the index. So it doesn't depend on the user query, but will return all the nodes currently in the index.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57896d05-1983-423f-a433-576a1e255c87",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/summary_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081185d0-3c90-4034-8e2b-1c94d9574b5b",
   "metadata": {},
   "source": [
    "### Setting Them Both Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a69f3f3-1f62-441a-98a2-fcc96ab53cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eab598-d3d0-4e04-80b9-0ecaa31d1f15",
   "metadata": {},
   "source": [
    "Now let's turn these indexes into **query engines** and then **query tools**:\n",
    "\n",
    "* Each query engine represents an overall query interface over the data that's stored in that index. It **combines retrieval with LLM synthesis**.\n",
    "\n",
    "* Each query engine is good for certain type of questions - great use case for a **router** which can route dynamically between these different query engines.\n",
    "\n",
    "* A **query tool** is now just a query engine with metadata, specifically a description of what types of questions the tool can answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd4b68ea-3b14-46aa-9a9f-7f51f7316ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbbac9-d905-4ad7-bea6-d568945b1b86",
   "metadata": {},
   "source": [
    "* Can see that the query engines are derived from each of the indexes.\n",
    "\n",
    "* Can see we use `use_async=True` for the summary query engine to basically enforce faster query generation by allowing async capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409d200-14d3-4ff8-83bd-123066971448",
   "metadata": {},
   "source": [
    "Next a **query tool** is just a query engine with some metadata -specifically a description of what type of question(s) that tool can answer.\n",
    "\n",
    "We'll define a **query tool** for both the *summary* and *vector* query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4122022a-ba92-4ee6-9564-774fe6976a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to `Assignment P1 CS 6675: Advanced Internet Systems and Applications report.`\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from `Assignment P1 CS 6675: Advanced Internet Systems and Applications report.`\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87127dc-ad18-46a7-b884-c306df67a069",
   "metadata": {},
   "source": [
    "### Selectors\n",
    "\n",
    "Now ready to define our **Router**. LlamaIndex provides several different types **Selectors** to enable you to build a router, each of these selectors have distinct attributes.\n",
    "\n",
    "There are several selectors available:\n",
    "\n",
    "* The **LLM selectors** use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.\n",
    "\n",
    "* The **Pydantic selectors** use the OpenAl Function\n",
    "Calling API to produce pydantic selection objects, rather than parsing raw JSON.\n",
    "\n",
    "\n",
    "\n",
    "For each of these kinds of selectors, also have dynamic capabilities to select one index to route to, or actually multiple.\n",
    "\n",
    "Let's try an LLM powered single selector - called `LLMSingleSelector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb41028-989f-4cba-804e-aa6cefa1006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07850b46-ed0b-4991-a643-795366864ac6",
   "metadata": {},
   "source": [
    "We import two modules:\n",
    "* **RouterQueryEngine**\n",
    "* **LLMSingleSelector**\n",
    "\n",
    "`RouterQueryEngine` takes in **selector type** as well as a set of **query engine tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f4afd-4af9-4cc9-a719-8ec64aa35c72",
   "metadata": {},
   "source": [
    "Let's test out some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67644323-cc46-40b5-b9e6-2327ad051c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The summary of the document is related to summarization questions, making choice 1 the most relevant..\n",
      "\u001b[0mThe document discusses various Internet-fueled innovations such as cloud computing, smart-watches, and community-based chat platforms. It highlights how these innovations automate tasks for users and involve cloud computing in some way. Additionally, it explains the differences between smart-watches and other innovations, emphasizing the unique hardware aspect of smart-watches. The document also delves into the concepts of surface web and deep web activities, explaining the challenges of crawling dynamic URLs in the deep web. Lastly, it touches upon limitations of a specific crawler project and proposes strategies for designing a crawler for a university website.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa163fb-480f-4969-b269-f47a926dc665",
   "metadata": {},
   "source": [
    "Verbose output allows us to view the intermediate steps that have been taken. We see that the output includes:\n",
    "\n",
    "* Selecting query engine 0: Useful for summarization questions. This means the first option was picked to help answer this question. As a result - you are able to get back a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ae80-abb3-4325-83b4-648c2c71af23",
   "metadata": {},
   "source": [
    "The response comes with **sources**. Let's take a look using `response.sources_nodes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a079b9-9253-41b1-a8ba-2e2e3000b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc8cf6-9424-4d64-b93d-db31e88f86c8",
   "metadata": {},
   "source": [
    "Exactly equal to the number of chunks of the entire document. We see the summary query engine must have been getting call - because the **summary query engine returns all the chunks corresponding to the items within its index**.\n",
    "\n",
    "Let's take a look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8709fb2d-ee50-4c6a-a8d6-62fa8ef6db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the report, which includes information about the author..\n",
      "\u001b[0mMher Arabian.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who write this paper?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317d4c2-703d-497c-aa67-67751c2a8d05",
   "metadata": {},
   "source": [
    "* Used vector search tool, as opposed to the summary tool.\n",
    "\n",
    "* Here focus is on retrieving specific context from the PDF - especially when information is located within a paragraph of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4345f-c4c5-41ac-8111-80601834cce4",
   "metadata": {},
   "source": [
    "**Puting everything together**: \n",
    "* Single helper function that takes in a file path and builds a router query engine with both vector search and summarization over it.\n",
    "* See `get_router_query_engine` in *utils.py* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4700781-82ee-46fd-acc1-9f04881f6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "# Load paper \"METAGPT: META PROGRAMMING FOR A MULTI-AGENT COLLABORATIVE FRAMEWORK\"\n",
    "file_path = os.path.join('data/pdfs', 'metagpt.pdf')\n",
    "query_engine = get_router_query_engine(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ca2d7a-fdc4-4299-ae17-f933dbf49834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results demonstrate the effectiveness of MetaGPT in addressing challenges related to context utilization, reducing hallucinations in software generation, and managing information overload. The study highlights how MetaGPT's unique designs successfully unfold natural language descriptions accurately, maintain information validity in lengthy contexts, and reduce code hallucination problems. Additionally, the study showcases the use of a global message pool and subscription mechanism to streamline communication and filter out irrelevant information, ultimately enhancing the relevance and utility of the generated information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10ca144-8e5e-49d5-b7a3-dc3aa86f0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
      "\u001b[0mThe paper introduces MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). MetaGPT focuses on improving code generation quality through role specialization, workflow management, and efficient communication mechanisms. It outperforms existing approaches in various benchmarks, demonstrating its effectiveness in software development tasks. The framework uses natural language programming to generate software code by involving different agents like the Architect, Engineer, and QA Engineer. MetaGPT aims to simplify the process of transforming abstract requirements into detailed designs by efficiently dividing tasks. The paper also addresses challenges such as reducing code hallucinations and handling information overload, while discussing ethical concerns like skill obsolescence, transparency, and privacy in the context of natural language programming's potential to enhance software development processes.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Summarize the paper for me.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64ef51-56f4-49df-994b-61c1ea6443dc",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df700da-72bc-4e10-8953-493d865ffad5",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "* In a basic RAG pipeline, LLMs are only use for synthesis. The previous lesson showed you how to use LLMs to make a decision by picking a choice of different pipelines.\n",
    "\n",
    "* This is a simplified form of tool calling. In this lesson we will learn how to use an LLM to not only pick a function to execute, but also infer an argument to pass to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191be46f-1dc6-47bf-be01-03d920ff0829",
   "metadata": {},
   "source": [
    "One of the Promises of LLMs:\n",
    "\n",
    "* Ability to take actions/interact with external environment. For this, we need a good interface for the LLMs to use.\n",
    "\n",
    "* We call this **Tool Calling**.\n",
    "\n",
    "\n",
    "* In previous lesson we saw how to use LLMs in a slightly more sophisticated matter than just synthesis. By using it to pick the best query pipeline to answer the user query. In this lesson we will learn how to use LLMs to not only pick a function to execute - but also infer an argument to pass to that function.\n",
    "\n",
    "* Allows LLM to figure out how to use a vector DB, instead of just consuming its outputs.\n",
    "\n",
    "* Tool calling enables LLMs to interact with external environments through a dynamic interface where tool calling not only helps choosing the appropriate tool but also infer necessary arguments for execution.\n",
    "\n",
    "\n",
    "* Tool calling adds a layer of query understanding on top of a RAG pipeline, enable users to ask complex queries and get\n",
    "back more precise results.\n",
    "\n",
    "\n",
    "* Final result: Users are able to ask more questions, and get back more precise results than standard RAG techniques thru tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb728d-84b5-4d0c-8c03-34099933e892",
   "metadata": {},
   "source": [
    "Let's see how to define a tool interface from a python function. LLM will infer the parameters from the signature of the python function using LlamaIndex abstractions.\n",
    "\n",
    "Let's see how Tool Calling works using two toy calculator functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca65a631-171c-4f76-bc85-7813a96aea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a378298-a41c-475b-8084-613584ae7f39",
   "metadata": {},
   "source": [
    "Core abstraction is **FunctionTool**, wraps any given python function you feed it, here it takes in both the add function as well as the mystery function.\n",
    "\n",
    "You see both have type annotations for both x,y variables and docstrings. These things are not just for stylistic purposes, they will actually be used as a prompt for the LLM.\n",
    "\n",
    "LlamaIndex FunctionTools integrate natively with the function calling capabilities of many different LLMs.\n",
    "\n",
    "To pass the tools to an LLM, have to import the LLM module and call `predict_and_call`.\n",
    "\n",
    "\n",
    "Here we import the OpenAI module explicitly, and we see the model is 3.5 turbo - we call the `predict_and_call` function on top of the LLM.\n",
    "\n",
    "What predict and call does:\n",
    "* Takes in a set of tools, as well as an input prompt string, or a series of chat messages, and then its able to both make a decision of the tool to call, as well as call the tool itself and get back the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72f769d1-e603-4267-aa29-f6df1c701113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4743212a-481d-4df0-a92e-f1f21db39330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"x\": 10, \"y\": 1}\n",
      "=== Function Output ===\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of add function with 10 and 1\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24752cc-23ac-4237-b0b5-cb4c12742e6b",
   "metadata": {},
   "source": [
    "We see intermediate steps here - calling function mystery with the arguments x=2 and y=9. We see called the right tools, and inferred the right parameters. The output is 121.\n",
    "\n",
    "**Note**: This simple example is effectively an expanded version of the router. Not only does the LLM pick the tool but also decides what params to give it.\n",
    "\n",
    "Let's define a slightly more sophisticated agentic layer on top of vector search. Not only can the LLM choose vector search, but **we can also get it to infer metadata filters** - which is a structured list of tags that helps return a more precised set of search results.\n",
    "\n",
    "Let's pay attention to the actual nodes themselves - the chunks. Because we'll take a look at the actual metadata attached to these chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719387a1-2a02-4a3f-9fc5-f421565d1368",
   "metadata": {},
   "source": [
    "Again we will use the `SimpleDirectoryReader` module to load in the parsed reprentation of this PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3216e10d-fe20-40be-9b8c-795925c6698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "file_path = os.path.join('data/pdfs', \"metagpt.pdf\")\n",
    "documents = SimpleDirectoryReader(input_files=[file_path]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab189c-3a0f-48ec-85f9-5c44928e2370",
   "metadata": {},
   "source": [
    "Next will split these documents into a set of even chunks, with a chunk size of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c0b2d8-a927-4752-9434-67567df10b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba2b3d68-9dbb-43e3-8ba4-b46d2f8d1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f667870-33b1-475b-9c55-483f0ddd4e7a",
   "metadata": {},
   "source": [
    "Each node represents a chunk, let's look at content of an example chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f598abc-f2b4-4ebf-9752-daab8f6ac6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: data/pdfs/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-06-07\n",
      "last_modified_date: 2024-06-07\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# \"all\" is a special setting - not just enable to print content of node\n",
    "# but also the metadata attached to the doc, which is propagated to every node\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9da219cc-9423-42f1-8936-3f6e0413a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint\n",
      "Figure 2: An example of the communication protocol (left) and iterative programming with exe-\n",
      "cutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\n",
      "They can also subscribe to relevant messages based on their profiles. Right : After generating the\n",
      "initial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\n",
      "messages stored in memory and compares them with the PRD, system design, and code files.\n",
      "3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\n",
      "MetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\n",
      "vides an explanation of role specialization, workflow and structured communication in this frame-\n",
      "work, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\n",
      "presents a communication protocol that enhances role communication efficiency. We also imple-\n",
      "ment structured communication interfaces and an effective publish-subscribe mechanism. These\n",
      "methods enable agents to obtain directional information from other roles and public information\n",
      "from the environment. Finally, we introduce executable feedback—a self-correction mechanism for\n",
      "further enhancing code generation quality during run-time in Sec. 3.3.\n",
      "3.1 A GENTS IN STANDARD OPERATING PROCEDURES\n",
      "Specialization of Roles Unambiguous role specialization enables the breakdown of complex work\n",
      "into smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\n",
      "oration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\n",
      "specific issues.\n",
      "In a software company, a Product Manager typically conducts business-oriented analysis and derives\n",
      "insights, while a software engineer is responsible for programming. We define five roles in our\n",
      "software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\n",
      "shown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\n",
      "goal, and constraints for each role. We also initialize the specific context and skills for each role.\n",
      "For instance, a Product Manager can use web search tools, while an Engineer can execute code, as\n",
      "shown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\n",
      "Every agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\n",
      "servations ( e.g.,, messages from other agents). These messages can either directly trigger actions or\n",
      "assist in finishing the job.\n",
      "Workflow across Agents By defining the agents’ roles and operational skills, we can establish\n",
      "basic workflows. In our work, we follow SOP in software development, which enables all agents to\n",
      "work in a sequential manner.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# all is a special setting - not just enable to print content of node\n",
    "# but also the metadata attached to the doc, which is propagated to every node\n",
    "print(nodes[4].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06500d93-dbe6-4ad0-9a15-dcb43586a066",
   "metadata": {},
   "source": [
    "Notice how it added a `page_label` annotation to each chunk.\n",
    "\n",
    "Next let's define a **vector store** index over these nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92d90a14-a3b6-4dc3-93e7-29f7705b0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf120c7-2fd9-479d-88d5-c3cacc859a86",
   "metadata": {},
   "source": [
    "### Querying RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5684c-7e16-4d02-9eb7-a0edf9334686",
   "metadata": {},
   "source": [
    "This will build a RAG indexing pipeline over these nodes. Will add an embedding for each node, and it will get back a query engine.\n",
    "\n",
    "Differently from last time, we can try querying this RAG pipeline via metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d6e263f-21a1-48d5-b7e1-74cf7d7c2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some of the high-level results of MetaGPT?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "346f8a90-7311-4efb-957b-4b36ea15e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality. Additionally, MetaGPT demonstrates a 100% task completion rate in experimental evaluations, showcasing its robustness and efficiency in terms of time and token costs.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfdd25e0-5b77-4a37-bbbb-57018a4eaea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069ae2c-d382-4686-b1ef-960e90e8f5d4",
   "metadata": {},
   "source": [
    "### Enhanced Data retrieval\n",
    "\n",
    "* Integrating Metadata Filters into a retrieval tool function.\n",
    "\n",
    "* This function enables more precise retrieval retrieval by accepting a query string and optional metadata filters, such as page numbers.\n",
    "\n",
    "* The LLM can intelligently infer relevant metadata filters (e.g., page numbers) based on the user's query.\n",
    "\n",
    "* You can define different type of metadata filters like section IDs, headers, or footers,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1436c6-6a82-496d-9f8a-0465cfe7b76e",
   "metadata": {},
   "source": [
    "Function below takes in a query as well as page numbers. This allows you to perform a vector search over an index, along with specifying page numbers as a metadata filter.\n",
    "\n",
    "At the very end we define a vector query tool. We pass in the vector query function into the vector query tool - which allows us to then use it with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462c525b-4e3c-4ca7-a89d-cb10cfc44524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dc744-c0b4-41e7-8187-bb8219015f56",
   "metadata": {},
   "source": [
    "Let's call this tool with an LLM. It should be able to infer both the string as well as the metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20e73901-f45c-49c3-aa07-7b45693c06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating a 100% task completion rate in experimental evaluations.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"What are the high-level results of MetaGPT as described in page 2\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256ac0d-ef2d-40d9-8447-97b767efdc90",
   "metadata": {},
   "source": [
    "LLM formulates the right query, as well as specify the page numbers. We get back the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4e6ff-71d4-49c2-a6c3-b61c0e0f78be",
   "metadata": {},
   "source": [
    "Let's verify the source nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56ee1014-4b66-4cd0-84da-83547f275523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38b03d-bc87-4197-818e-4f43d4c5b44d",
   "metadata": {},
   "source": [
    "Finally, we can bring in the summary tool from the router example of the first lesson and we can combine that with the vector tool, to create this **overall tool picking system**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3330456-53be-4989-bdbe-415127c3dc0f",
   "metadata": {},
   "source": [
    "This code sets up a summary index over the same set of nodes and wraps it in a summary tool similar to lesson 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab41309-4cb9-4665-9e0e-6762fcc4eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4445804-47ca-45b4-a9e0-dca914c5c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev on the challenging SoftwareDev dataset in nearly all metrics. For example, MetaGPT achieves a higher score in executability, takes less time for execution, requires more tokens for code generation but needs fewer tokens to generate one line of code compared to ChatDev. Additionally, MetaGPT demonstrates superior performance in code statistics and the cost of human revision when compared to ChatDev.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c405de-0568-4c29-b773-793c095b0d88",
   "metadata": {},
   "source": [
    "Now LLM has slightly harder task - must pick the right tool in addition to inferring the function parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe34668-e1ed-47f4-bac6-e0978cf9eb3c",
   "metadata": {},
   "source": [
    "Let's verify this by printing out the sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74953632-927b-4ea4-9a4e-94ac3273dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'data/pdfs/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce328ffe-5bb7-43a2-ab8e-07a790696eb7",
   "metadata": {},
   "source": [
    "Let's now ask a question, to show that the LLM can still pick the summary tool when necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc95e0e1-2559-4ed3-b959-5b3eee853bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It introduces role specialization, structured communication interfaces, and executable feedback mechanisms to improve code generation quality. In experiments, MetaGPT surpassed previous approaches in various benchmarks, showcasing its effectiveness in software development tasks. The framework also introduces innovative concepts like self-improvement mechanisms and multi-agent economies for future research and development. The Architect agent devises technical specifications based on the Product Requirement Document (PRD), while the Project Manager breaks down tasks and assigns them to Engineers. The Engineer agent requires fundamental development skills, and the QA Engineer generates unit test code to ensure high-quality software. Ultimately, MetaGPT generates a functional application called \"Drawing App\" using Python's Tkinter and PIL (Pillow) libraries for GUI and color selection functionality, respectively.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is a summary of the paper?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a1f7f-597c-4d73-ab7d-b25f487d1b33",
   "metadata": {},
   "source": [
    "Next lesson: How to build a full agent over a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e8f53-a425-4822-9b71-64300c9be59d",
   "metadata": {},
   "source": [
    "## Building an Agent Reasoning Loop\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6967e37-69f2-47ed-b5e5-a87c2bc51494",
   "metadata": {},
   "source": [
    "In this lesson, will learn how to define a complete agent reasoning loop. Instead of tool calling in a single-shot setting, an agent is able to reason over tools in multiple steps.\n",
    "\n",
    "Will use the function calling agent implementation which is an agent that natively integrates with the function calling capabilities of LLMs.\n",
    "\n",
    "Let's have some fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1aa2e7b-8a38-4edd-a2c6-a9f8ab22719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Accessing the environment variable\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded properly\n",
    "if openai_api_key is not None:\n",
    "    print(\"OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load OPENAI_API_KEY. Please check if it is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4031e30-f418-4467-957b-a9da932de71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary setup to run in notebook environment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c3a1d-6251-4bf0-90ad-9c26a2088959",
   "metadata": {},
   "source": [
    "Let's also setup the autoretrieval vector seach tool and summarization tool from the last lesson.\n",
    "\n",
    "Made it easy with a function in the utils file. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ba7052f-1143-4bc5-bbe2-58a068bb2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_doc_tools\n",
    "\n",
    "# make sure you have a valid pdf file in this directory\n",
    "file_path = os.path.join('data/pdfs', 'metagpt.pdf')\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(file_path, \"metagpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f690b31-d978-4d33-9a4a-1782bfe74802",
   "metadata": {},
   "source": [
    "### High-level Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46df6c-cc71-461f-a858-e93e1cffda5d",
   "metadata": {},
   "source": [
    "Let's setup our function calling agent.\n",
    "\n",
    "\n",
    "In LlamaIndex, an agent consists of two main components:\n",
    "1. **Agent Worker**: Responsible for executing the next step of a given agent.\n",
    "2. **Agent Runner**: Overall task dispatcher, responsible for creating a task, orchestrating runs of agent workers on top of a given task, and being able to return back the final response to the user.\n",
    "\n",
    "\n",
    "![Agent Intro](imgs/agent_intro.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "606c0d2c-e671-444e-86aa-ccf40628964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55accd93-5dea-4f77-8cf7-4e5207742079",
   "metadata": {},
   "source": [
    "`FunctionCallingAgentWorker` primary responsibility as given the existing converstion history, memory, and any past state, along with the current user input: use function calling to decide the next tool to call, call that tool and decide whether or not to return a final response.\n",
    "\n",
    "The overall agent interface is behind the agent runner, and that's what we're gonna use to query the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11763ad4-c58e-4a1c-aec6-fbf4206e5e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and tasks assigned to them within the collaborative framework to streamline the software development workflow and ensure successful project completion. The Product Manager focuses on requirements and user stories, the Architect designs the system architecture, the Project Manager handles task allocation, the Engineer implements the code, and the QA Engineer ensures software quality through testing and bug fixing.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"how agents communicate with each other in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT communicate with each other through structured communication interfaces and a shared message pool. The structured communication interfaces define the format and schema for each role, ensuring that individuals provide necessary outputs based on their specific roles and contexts. Agents publish structured messages in a shared message pool, allowing all agents to exchange messages directly. Additionally, agents can subscribe to relevant messages based on their role profiles, enhancing communication efficiency and reducing information overload.\n",
      "=== LLM Response ===\n",
      "In MetaGPT, the agent roles include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. They communicate with each other through structured communication interfaces and a shared message pool. The structured interfaces define the format for each role, and agents publish messages in the shared pool for direct exchange. Agents can subscribe to relevant messages based on their roles, improving communication efficiency and reducing information overload.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9f465-3f2e-4c2f-bddb-d10ac794d6d1",
   "metadata": {},
   "source": [
    "When you run a multi-step query like this. You want to make sure that you're actually able to trace the sources. \n",
    "\n",
    "So luckily, similar to previous lessons, can look at `response.source_nodes`. Take a look at the content of these nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f38403f6-c9f9-44b6-8083-63975f9d11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 7\n",
      "file_name: metagpt.pdf\n",
      "file_path: data/pdfs/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-06-07\n",
      "last_modified_date: 2024-06-07\n",
      "\n",
      "Preprint\n",
      "wareDev: (1) HumanEval includes 164 handwritten programming tasks. These tasks encompass\n",
      "function specifications, descriptions, reference codes, and tests. (2) MBPP consists of 427 Python\n",
      "tasks. These tasks cover core concepts and standard library features and include descriptions, ref-\n",
      "erence codes, and automated tests. (3) Our SoftwareDev dataset is a collection of 70 representa-\n",
      "tive examples of software development tasks, each with its own task prompt (see Table 8). These\n",
      "tasks have diverse scopes (See Figure 5), such as mini-games, image processing algorithms, data\n",
      "visualization. They offer a robust testbed for authentic development tasks. Contrary to previous\n",
      "datasets (Chen et al., 2021a; Austin et al., 2021), SoftwareDev focuses on the engineering aspects.\n",
      "In the comparisons, we randomly select seven representative tasks for evaluation.\n",
      "Evaluation Metrics For HuamnEval and MBPP, we follow the unbiased version of Pass @ kas\n",
      "presented by (Chen et al., 2021a; Dong et al., 2023), to evaluate the functional accuracy of the top-k\n",
      "generated codes: Pass @ k=EProblems\u0014\n",
      "1−(n−c\n",
      "k)\n",
      "(n\n",
      "k)\u0015\n",
      ".\n",
      "For SoftwareDev, we prioritize practical use and evaluate performance through human evaluations\n",
      "(A, E) or statistical analysis (B, C, D): (A)Executability: this metric rates code from 1 (failure/non-\n",
      "functional) to 4 (flawless). ‘1’ is for non-functional, ‘2’ for runnable but imperfect, ‘3’ for nearly\n",
      "perfect, and ‘4’ for flawless code. (B)Cost: the cost evaluations here include the (1) running time,\n",
      "(2) token usage, and (3) expenses. (C)Code Statistics: this includes (1) code files, (2) lines of code\n",
      "per file, and (3) total code lines. (D)Productivity: basically, it is defined as the number of token\n",
      "usage divided by the number of lines of code, which refers to the consumption of tokens per code\n",
      "line. (E)Human Revision Cost: refers to times of manual code corrections, which tackle problems\n",
      "like package import errors, incorrect class names, or incomplete reference paths. Typically, each\n",
      "correction involves up to 3 lines of code.\n",
      "Baselines We compare our method with recent domain-specific LLMs in the code generation field,\n",
      "including AlphaCode (Li et al., 2022), Incoder (Fried et al., 2022), CodeGeeX (Zheng et al., 2023),\n",
      "CodeGen (Nijkamp et al., 2023), CodeX (Chen et al., 2021a), and CodeT (Chen et al., 2022) and\n",
      "general domain LLMs such as PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023). Several\n",
      "results of baselines (such as Incoder, CodeGeeX) are provided by Dong et al. (2023). In HumanEval\n",
      "and MBPP, we slightly modified the prompts to align with response format requirements. These\n",
      "modifications aim to address format-specific issues (i.e., Python problems). With the SoftwareDev\n",
      "benchmark, we provide a comprehensive comparison between MetaGPT, AutoGPT (Torantulino\n",
      "et al., 2023), LangChain (Chase, 2022) with Python Read-Eval-Print Loop (REPL) tool3, Agent-\n",
      "Verse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\n",
      "4.2 M AINRESULT\n",
      "AlphaCode(1.1B)\n",
      "Incoder (6.7B)\n",
      "CodeGeeX (13B)17.1\n",
      "—15.2 17.6 18.926.9\n",
      "CodeGeeX-Mono(16.1B)32.938.6\n",
      "GPT-467.0\n",
      "—\n",
      "MetaGPT\n",
      "(w/o Feedback)81.7 82.3Pass@1 of MBPP  and HumanEval (%)\n",
      "PaLM Coder(540B)36.047.0\n",
      "Codex (175B)47.058.1\n",
      "Codex + CodeT65.8 67.7\n",
      "HumanEval\n",
      "MBPP\n",
      "MetaGPT85.9 87.7\n",
      "Figure 4: Pass rates on the MBPP and HumanEval with a single attempt.\n",
      "Performance Figure 4 demonstrates that MetaGPT outperforms all preceding approaches in both\n",
      "HumanEval and MBPP benchmarks.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e0d54-4d18-4893-b944-4dc44a66028b",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/full_agent_reasoning_loop.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82a88a-d5ca-431c-a73b-ef1fcf13e704",
   "metadata": {},
   "source": [
    "Calling `agent.query()` allows you to query the agent in a one-off manner, but does not preserve state. \n",
    "\n",
    "So now let's try maintaining conversation history over time.\n",
    "\n",
    "The agent is able to maintain chats in a conversational memory buffer.\n",
    "\n",
    "The memory module can be customized - by default is a flat list of items that's a rolling buffer depending on the size of the context window of the LLM.\n",
    "\n",
    "Therfore, when the agent decides to use a tool, it not only uses a current chat, but also previously convo history to take the next step/perform next action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c8e4bf9-2ddd-4596-b5d8-e7abcb57d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation datasets used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and a self-generated SoftwareDev dataset. The HumanEval dataset consists of 164 handwritten programming tasks, while MBPP consists of 427 Python tasks. The SoftwareDev dataset comprises 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and a self-generated SoftwareDev dataset. HumanEval consists of 164 handwritten programming tasks, MBPP includes 427 Python tasks, and the SoftwareDev dataset comprises 70 representative software development tasks covering various scopes.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5570f-013e-4618-94b9-a8cd13f756fc",
   "metadata": {},
   "source": [
    "Now let's ask a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a07d2642-29ee-4ca2-b3fe-7a72f4711522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the results over one of the above datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"results over HumanEval dataset\", \"page_numbers\": [\"7\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieved a Pass rate of 85.9% and 87.7% over the HumanEval dataset.\n",
      "=== LLM Response ===\n",
      "MetaGPT achieved a Pass rate of 85.9% and 87.7% over the HumanEval dataset.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me the results over one of the above datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa4542-1098-42ef-be06-1121406b4dbe",
   "metadata": {},
   "source": [
    "### Low-level Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d0e81-0b1b-4b20-b657-6d66deed28c3",
   "metadata": {},
   "source": [
    "Just provided a nice, high-level interface for interacting with an agent.\n",
    "\n",
    "Next section will show you capabilities that let you step through and control the agent in a much more granular fashion. Not only allows you to create a higher level research assistant over your RAG pipelines, but also debug/control it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7a90c-316b-4a47-978f-51b0534e078a",
   "metadata": {},
   "source": [
    "![Router Engine](imgs/agent_control.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fc0cf-8522-4ab1-9ed1-6039c5733749",
   "metadata": {},
   "source": [
    "Having this low-level agent interace is powerful for two main reasons:\n",
    "1. **Allows developers of agents to have greater transparency/visibility into what's actually going on under the hood**, especially if the agent isn't working the first time around, can go in, trace thru execution of the agent, see where it is failing, try out different inputs to if that actually modifies the agent execution into a correct response\n",
    "\n",
    "2. **Enable useful UX's when building a product experience around this core agentic capability**. For instance, let's say you want to listen to human feedback in the middle of agent execution, as opposed to only after the agent execution is complete for a given task. Then can imagine creating some sort of async queue, where you're able to listen to inputs from humans throughout the middle of agent execution and if human input comes in, can actually interrupt and modify the execution of an agent as it's going on thru a larger task, as opposed to having to wait until the agent's task is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c5ef3d1-c8ed-4a14-9def-e46676a8e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef67ad-4090-43cd-b65a-94b4f55ada28",
   "metadata": {},
   "source": [
    "Let's start using the low-level API. First, will create a task object from the user query, and then will start running thru steps or even interjecting our own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18f748-cd26-4b44-b653-234c2792e901",
   "metadata": {},
   "source": [
    "Let's try executing a single step of this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebcb8752-9c6d-4763-9500-466b3933fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce1f4e-8367-46cc-879e-570d4cdec787",
   "metadata": {},
   "source": [
    "We created a task for this agnet, this will return a **task object** which contains the input as well **additional state in the task object**.\n",
    "\n",
    "Now let's try executing a single step of this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4161123-e479-4b48-b8cc-18ba1e346f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT are Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities within the software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the system architecture and interfaces, the Project Manager allocates tasks, the Engineer implements the code based on specifications, and the QA Engineer ensures code quality through testing and bug fixing. These roles work together in a structured workflow to successfully complete complex software development projects within the MetaGPT framework.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe3bee-21fe-478d-a351-20f2ac392fc2",
   "metadata": {},
   "source": [
    "The agent executes a step of this task through the task ID and give you back a step output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d31488-18d3-439b-aaae-a4e2cf109f2f",
   "metadata": {},
   "source": [
    "It calls the summary tool with the input: \"agent roles in MetaGPT\", which is the very first part of this question. And then it stops there.\n",
    "\n",
    "When we inspect the logs, and the output of the agent, we see that the first part was actually executed. So we call `agent.get_completed_steps()` with the *task_id*, and we're able to look at the num completed for the task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93dae43e-80a5-4fb3-94e4-7c36ab2f6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task e2612548-b67d-4c38-9448-8fa2c58c31f8: 1\n",
      "The agent roles in MetaGPT are Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities within the software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the system architecture and interfaces, the Project Manager allocates tasks, the Engineer implements the code based on specifications, and the QA Engineer ensures code quality through testing and bug fixing. These roles work together in a structured workflow to successfully complete complex software development projects within the MetaGPT framework.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd294e27-be20-4723-9067-71f0f1cd9b09",
   "metadata": {},
   "source": [
    "We see that one step has been completed and this is the current output so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19e055-bbb4-4fe4-a0a0-e30041251bf4",
   "metadata": {},
   "source": [
    "We can also take a look at any upcoming steps for the agent thru `agent.get_upcoming_steps()` and passing the *task_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "afb92c91-b5cf-43fc-97ab-30a0781384ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num upcoming steps for task e2612548-b67d-4c38-9448-8fa2c58c31f8: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='e2612548-b67d-4c38-9448-8fa2c58c31f8', step_id='d1c8481c-24a2-4f89-8d3d-d8c3e03841a3', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9664343-4359-421c-8e0e-65224a25539d",
   "metadata": {},
   "source": [
    "We see it's also 1, and we're able to look at a `TaskStep` object with a *task id*, as well as an existing input. This input is currently `None` because the way the agent works is it actually just **autogenerates the action from the conversation history**, and doesn't need to generate an additional external input.\n",
    "\n",
    "The nice thing about this debugging interface is that if you wanted to pause execution now, you can. You can take the intermediate results without completing the agent flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f55ced-166f-4251-8fcf-23ab40a92afc",
   "metadata": {},
   "source": [
    "Let's run the next two steps and actually try and drafting user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c32860e-b402-4ead-983d-f02580b26ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What about how agents share information?\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"how agents share information in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT share information through a structured communication protocol that includes structured communication interfaces and a publish-subscribe mechanism. This mechanism allows agents to exchange structured messages in a shared message pool, enabling transparent access to information from other agents. Agents can subscribe to specific information based on their role profiles, ensuring efficient communication and task-related information dissemination. The global message pool centralizes information exchange, and the subscription mechanism filters out irrelevant data, ensuring that agents receive only the most relevant and useful information. This structured approach helps streamline communication and enhance the effectiveness of information sharing among agents in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"What about how agents share information?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba11cb2-623a-4ca5-a1c8-20b9116f14e9",
   "metadata": {},
   "source": [
    "Not part of the original task query, but by injecting this, can actually modify agent execution to give you back the result that you want.\n",
    "\n",
    "We'll see that we added the *user message* to **memory**. Next call is \"how agents share information in MetaGPT\".\n",
    "\n",
    "We see here it's able to give back a response. Overall task is complete, just need to run 1 final step to synthesize the answer, and to double check that this output is the last step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f8bf9bd-7061-4a2d-b91f-8d69b19300f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Agents in MetaGPT share information through a structured communication protocol that includes structured communication interfaces and a publish-subscribe mechanism. This mechanism allows agents to exchange structured messages in a shared message pool, enabling transparent access to information from other agents. Agents can subscribe to specific information based on their role profiles, ensuring efficient communication and task-related information dissemination. The global message pool centralizes information exchange, and the subscription mechanism filters out irrelevant data, ensuring that agents receive only the most relevant and useful information. This structured approach helps streamline communication and enhance the effectiveness of information sharing among agents in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1de7fb58-4b81-4299-825d-a93e9697d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520b11c-bd7f-4e28-b3bd-cdc5ed829782",
   "metadata": {},
   "source": [
    "We get back the answer, this is the last step. To translate this into an **agent response**, similar to what we've seen in some of the previous notebook cells, then all you have you do is call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "272aca1c-3e9e-4dd7-8366-674466febe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "486ec7ec-92d5-4b7b-a9ec-c9418284bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents in MetaGPT share information through a structured communication protocol that includes structured communication interfaces and a publish-subscribe mechanism. This mechanism allows agents to exchange structured messages in a shared message pool, enabling transparent access to information from other agents. Agents can subscribe to specific information based on their role profiles, ensuring efficient communication and task-related information dissemination. The global message pool centralizes information exchange, and the subscription mechanism filters out irrelevant data, ensuring that agents receive only the most relevant and useful information. This structured approach helps streamline communication and enhance the effectiveness of information sharing among agents in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a680af-27ab-4a19-9e2f-c17da9ef5911",
   "metadata": {},
   "source": [
    "That's it! You've learned about: The high-level interface for an agent, as well as a low-level debugging interface.\n",
    "\n",
    "In the next lesson, we'll learn how to build an agent over multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ee2cc-f9ad-48b1-a333-061a071fd8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
